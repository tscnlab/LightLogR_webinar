[
  {
    "objectID": "beginner.html#preface",
    "href": "beginner.html#preface",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Preface",
    "text": "Preface\nWearables are increasingly used in research because they combine personalized, high‑temporal‑resolution measurements with outcomes related to well‑being and health. In sleep research, wrist‑worn actimetry is long established. As circadian factors gain prominence across disciplines, interest in personal light exposure has grown, spurring a variety of new devices, form factors, and sensor technologies. This trend also brings many researchers into settings where data from wearables must be ingested, processed, and analyzed. Beyond circadian science, measurements of light and optical radiation are central to UV‑related research and to questions of ocular health and development.\nLightLogR is designed to facilitate the principled import, processing, and visualization of such wearable‑derived data. This document offers an accessible entry point to LightLogR via a self‑contained analysis script that you can modify to familiarize yourself with the package. Full documentation of LightLogR’s features is available on the documentation page, including numerous tutorials.\nThis document is intended for researchers with no prior experience using LightLogR, and assumes general familiarity with the R statistical software, ideally in a data‑science context1."
  },
  {
    "objectID": "beginner.html#how-this-page-works",
    "href": "beginner.html#how-this-page-works",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "How this page works",
    "text": "How this page works\nThis document contains the script for the online course series as a Quarto script, which can be executed on a local installation of R. Please ensure that all libraries are installed prior to running the script.\nIf you want to test LightLogR without installing R or the package, try the script version running webR, for a autonymous but slightly reduced version.\nTo run this script, we recommend cloning or downloading the GitHub repository (link to Zip-file) and running beginner.qmd. Alternatively, you can download the main script, the preview functions, and the data separately - though this is more laborious and error‑prone. In both cases, you’ll need to install the required packages. A quick way is to run:\n\nrenv::restore()"
  },
  {
    "objectID": "beginner.html#installation",
    "href": "beginner.html#installation",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Installation",
    "text": "Installation\nLightLogR is hosted on CRAN, which means it can easily be installed from any R console through the following command:\n\ninstall.packages(\"LightLogR\")\n\nAfter installation, it becomes available for the current session by loading the package. We also require a number of packages. Most are automatically downloaded with LightLogR, but need to be loaded separately. Some might have to be installed separately on your local machine.\n\nlibrary(LightLogR) #load the package\nlibrary(tidyverse) #a package for tidy data science\nlibrary(gt) #a package for great tables\n#the following packages are needed for preview functions:\nlibrary(cowplot)\nlibrary(legendry)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\nlibrary(patchwork)\nlibrary(rlang)\nlibrary(glue)\nlibrary(gtExtras)\nlibrary(svglite)\n\n#the next two scripts will be integrated into the next release of LightLogR\n#but have to be loaded separately for now (≤0.9.3)\nsource(\"scripts/overview_plot.R\")\nsource(\"scripts/summary_table.R\")\n\n# Set a global theme for the background\ntheme_set(\n    theme(\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n)\n\nThat is all we need to get started. Let’s make a quick visualization of a sample dataset that comes preloaded with the package. It contains six days of data from a participant, with concurrent measurements of environmental light exposure at the university rooftop. You can play with the arguments to see how it changes the output.\n\nsample.data.environment |&gt; #sample data\n  gg_days(geom = \"ribbon\", \n          aes_fill = Id, \n          alpha = 0.6, \n          facetting = FALSE\n          ) |&gt; \n  gg_photoperiod(c(47.1, 9)) +\n  coord_cartesian(expand = FALSE)"
  },
  {
    "objectID": "beginner.html#import",
    "href": "beginner.html#import",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Import",
    "text": "Import\n\nFile formats\nTo work with LightLogR, we need some data from wearables. To the side are screenshots from three example formats to highlight the structure and differences. You can enlarge by clicking at them.\n\n\n\n\n\nActLumus file structure\n\n\n\n\n\nSpeccy file structure\n\n\n\n\n\nnanoLambda file structure\n\n\n\n\nImporting a file\nThese files must be loaded into the active session in a tidy format—each variable in its own column and each observation in its own row. LightLogR’s device‑specific import functions take care of this transformation. Each function requires:\n\nfilenames and paths to the wearable export files\nthe time zone in which the data were collected\n(optional) participant identifiers\n\nWe begin with a dataset bundled with the package, recorded with the ActLumus device. The data were collected in Tübingen, Germany, so the correct time zone is Europe/Berlin.\n\n#accessing the filepath of the package to reach the sample dataset:\nfilename &lt;- \n  system.file(\"extdata/205_actlumus_Log_1020_20230904101707532.txt.zip\", \n              package = \"LightLogR\")\n\n\ndataset &lt;- import$ActLumus(filename, tz = \"Europe/Berlin\", manual.id = \"P1\")\n\n\nSuccessfully read in 61'016 observations across 1 Ids from 1 ActLumus-file(s).\nTimezone set is Europe/Berlin.\n\nFirst Observation: 2023-08-28 08:47:54\nLast Observation: 2023-09-04 10:17:04\nTimespan: 7.1 days\n\nObservation intervals: \n  Id    interval.time     n pct  \n1 P1    10s           61015 100% \n\n\n\n\n\n\n\n\n\nThe import function also provides rich summary information about the dataset—such as the time span covered, sampling intervals, and an overview plot. Most import settings are configurable. To learn more, consult the function documentation online or via ?import. For a quick visual overview of the data across days, draw a timeline with gg_days().\n\ndataset |&gt; gg_days()\n\n\n\n\n\n\n\n\nWe will go into much more detail about visualizations in the sections below.\n\n\nImporting from a different device\nEach device exports data in its own format, necessitating device‑specific handling. LightLogR includes import wrapper functions for many devices. You can retrieve the list supported by your installed version with the following function:\n\nsupported_devices()\n\n [1] \"Actiwatch_Spectrum\"    \"Actiwatch_Spectrum_de\" \"ActLumus\"             \n [4] \"ActTrust\"              \"Circadian_Eye\"         \"Clouclip\"             \n [7] \"DeLux\"                 \"GENEActiv_GGIR\"        \"Kronowise\"            \n[10] \"LiDo\"                  \"LightWatcher\"          \"LIMO\"                 \n[13] \"LYS\"                   \"MotionWatch8\"          \"nanoLambda\"           \n[16] \"OcuWEAR\"               \"Speccy\"                \"SpectraWear\"          \n[19] \"VEET\"                 \n\n\nWe will now import from two other devices to showcase the differences.\n\nSpeccy\n\nfilename &lt;- \"data/Speccy.csv\"\ndataset &lt;- import$Speccy(filename, tz = \"Europe/Berlin\", manual.id = \"P1\")\n\n\nSuccessfully read in 1'336 observations across 1 Ids from 1 Speccy-file(s).\nTimezone set is Europe/Berlin.\n\nFirst Observation: 2023-08-31 09:34:26\nLast Observation: 2023-09-01 07:49:26\nTimespan: 22 hours\n\nObservation intervals: \n  Id    interval.time        n pct  \n1 P1    60s (~1 minutes)  1335 100% \n\n\n\n\n\n\n\n\n\n\ndataset |&gt; gg_days()\n\n\n\n\n\n\n\n\n\n\nnanoLambda\n\nfilename &lt;- \"data/nanoLambda.csv\"\ndataset &lt;- import$nanoLambda(filename, tz = \"Europe/Berlin\", manual.id = \"P1\")\n\n\nSuccessfully read in 19 observations across 1 Ids from 1 nanoLambda-file(s).\nTimezone set is Europe/Berlin.\n\nFirst Observation: 2020-04-13 13:38:54\nLast Observation: 2020-04-13 13:47:54\nTimespan: 9 mins\n\nObservation intervals: \n  Id    interval.time     n pct  \n1 P1    30s              18 100% \n\n\n\n\n\n\n\n\n\nIf we try to visualize this dataset as we have done above, we get an error.\n\ndataset |&gt; gg_day()\n\nThis is because many LightLogR functions default to the melanopic EDI variable[^3]. However, the nanoLambda export does not include this variable. Therefore, we must explicitly specify which variable to display. Let’s inspect the available variables:\n[^3] melanopic equivalent daylight-illuminance, CIE S026:2018\n\ndataset |&gt; names()\n\n [1] \"Id\"                   \"File_Name\"            \"Device_Address\"      \n [4] \"Sensor_ID\"            \"Datetime\"             \"Is_Saturate\"         \n [7] \"Integration_Time(ms)\" \"Wavelength\"           \"Spectrum\"            \n[10] \"PPFD_Spectrum\"        \"Photopic_Lux\"         \"Cyanopic_Lux\"        \n[13] \"Melanopic_Lux\"        \"Rhodopic_Lux\"         \"Chloropic_Lux\"       \n[16] \"Erythropic_Lux\"       \"PPFD\"                 \"PPFD-R\"              \n[19] \"PPFD-G\"               \"PPFD-B\"               \"CCT\"                 \n[22] \"CRI\"                  \"XYZ\"                  \"xy\"                  \n[25] \"uv\"                   \"file.name\"           \n\n\nYou can choose any numeric variable; here, we’ll use Melanopic_Lux, which is similar—though not identical—to melanopic EDI. To identify which argument to adjust, consult the function documentation:\n\n?gg_day()\n\nUse the y.axis argument to select the variable. Also update the axis title via y.axis.label; otherwise the default label will refer to melanopic EDI.\nBecause this dataset spans only a short interval of about 9 minutes, we’ll visualize it with gg_day(), which uses clock time on the x‑axis. There are a few other differences to gg_days(), which we will see in the sections below.\n\ndataset |&gt; gg_day(y.axis = Melanopic_Lux, y.axis.label = \"melanopic illuminance\")\n\n\n\n\n\n\n\n\nIn summary, importing from different devices is typically as simple as specifying the device name. Some devices require additional arguments; consult the ?import help for details.\n\n\n\nImporting more than one file\nIn typical studies, you’ll work with multiple participants, and importing each file individually is cumbersome. LightLogR supports batch imports; simply pass multiple files to the import function. In this tutorial, we’ll use three files from three participants, all drawn from the open‑access personal light‑exposure dataset by Guidolin et al. 20252. All data were collected with the ActLumus device type.\nWhen importing multiple files, keep the following in mind:\n\nAll files must originate from the same device type, share the same export structure, and use the same time‑zone specification. If they differ, import them separately.\nBe deliberate about participant‑ID assignment. The manual.id argument used above would assign the same ID to all imported data in a batch. If a file contains a column specifying the Id, you can point to that column; more often, the identifier is encoded in the filename. If you omit ID arguments, the filename is used as Id by default. Because filenames are often verbose, you will typically extract only the participant code. In our three example files, the relevant IDs are 216, 218, and 219.\n\n\nfilenames &lt;- list.files(\"data\", pattern = \"actlumus\", full.names = TRUE)\nfilenames\n\n[1] \"data/216_actlumus_Log_1382_20231009130709885.txt\"\n[2] \"data/218_actlumus_Log_1020_2023102309585329.txt\" \n[3] \"data/219_actlumus_Log_1382_20231023113046652.txt\"\n\n\nIf filenames follow a consistent pattern, you can instruct the import function to extract only the participant code from each name. In our case, the first three digits encode the ID. We can specify this with a regular expression: ^(\\d{3}). This pattern matches the first three digits at the start of the filename and captures them (^ = start of string, \\d = digit, {3} = exactly three, (& ) = encloses the part of the pattern we actually want). If you’re not familiar with regular expressions, they can look like a jumble of ASCII characters, but they succinctly express patterns. Large language models are quite good at proposing regexes and explaining their components, so consider prompting one when you need a new pattern. With that, we can import our files.\n\npattern &lt;- \"^(\\\\d{3})\"\ndataset &lt;- import$ActLumus(filenames, tz = \"Europe/Berlin\", auto.id = pattern)\n\n\nSuccessfully read in 184'333 observations across 3 Ids from 3 ActLumus-file(s).\nTimezone set is Europe/Berlin.\n\nFirst Observation: 2023-09-29 14:40:52\nLast Observation: 2023-10-23 11:29:10\nTimespan: 24 days\n\nObservation intervals: \n   Id    interval.time            n pct  \n 1 216   10s                  61760 100% \n 2 216   19s                      1 0%   \n 3 216   240718s (~2.79 days)     1 0%   \n 4 218   8s                       1 0%   \n 5 218   10s                  60929 100% \n 6 218   11s                      1 0%   \n 7 219   9s                       1 0%   \n 8 219   10s                  61634 100% \n 9 219   16s                      1 0%   \n10 219   583386s (~6.75 days)     1 0%   \n\n\n\n\n\n\n\n\n\nThe overview plot is now more informative: it shows how the datasets align across time and highlights extended gaps due to missing data. We will return to the terminology of implicit missingness shortly.\n\ndataset |&gt; gg_days()\n\n\n\n\n\n\n\n\nDirect plotting highlights the extended gaps in the recordings. We’ll apply a package function that removes days with insufficient coverage to address this. For now, we can ignore the details: any participant‑day with more than 80% missing data will be excluded.\n\ndataset_red &lt;-\n  dataset |&gt;\n  remove_partial_data(\n    Variable.colname = MEDI,\n    threshold.missing = 0.8,\n    by.date = TRUE,\n    handle.gaps = TRUE\n  )\n\ndataset_red |&gt; gg_days()\n\n\n\n\n\n\n\n\nThat concludes the import section of the tutorial; next, we turn to visualization functions. For simplicitly, we will only carry a small selection of variables forward. That increases the calculation speed of many functions. Feel free to choose a different set of variables.\n\ndataset &lt;- dataset |&gt; select(Id, Datetime, PIM, MEDI)\n\n\n\nHow to find the correct time zone name?\nA final note on imports: the function accepts only valid IANA time‑zone identifiers. You can retrieve the full list (with exact spellings) using:\n\nOlsonNames() |&gt; sample(5)\n\n[1] \"Europe/Vienna\"        \"Africa/El_Aaiun\"      \"Etc/GMT-6\"           \n[4] \"Asia/Urumqi\"          \"Pacific/Port_Moresby\""
  },
  {
    "objectID": "beginner.html#basic-visualizations",
    "href": "beginner.html#basic-visualizations",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Basic Visualizations",
    "text": "Basic Visualizations\nVisualization is central to exploratory data analysis and to communicating results in publications and presentations. LightLogR provides a suite of plotting functions built on ggplot2 and the Grammar of Graphics. As a result, the plots are composable, flexible, and straightforward to modify.\n\ngg_days()\ngg_days() displays a timeline per each Id. It constrains the x‑axis to complete days and, by default, uses a line geometry. The function works best for up to a handful of Id’s and 1-2 weeks of data at most.\n\ndataset_red |&gt; gg_days(aes_col = Id) #try interactive = TRUE\n\n\n\n\n\n\n\n\n\n\ngg_day()\ngg_day() complements gg_days() by focusing on individual calendar days. By default, it places all observations from a selected day into a single panel, regardless of source. This layout is configurable. For readability, gg_day() works best with ~1–4 days of data (at most about a week) to keep plot height manageable.\n\ndataset_red |&gt; \n  gg_day(aes_col = Id, \n         format.day = \"%A\", # switch from dates to week-days\n         size = 0.5, # reduce point size\n         x.axis.breaks = hms::hms(hours = c(0, 12))) + #12-hour grid \n  guides(color = \"none\") + # remove color legend\n  facet_grid(rows = vars(Day.data), cols = vars(Id), switch = \"y\") # Id x Day\n\n\n\n\n\n\n\n\n\n\ngg_overview()\ngg_overview() is invoked automatically by the import function but can also be called independently and customized. By default, each Id appears as a separate row on the y‑axis. For longitudinal datasets with large gaps between recordings, you can group observations (e.g., by a session variable) to distinguish distinct measurement periods (see margin figure). The function works nice for many participants and long collection periods, by setting their recording periods in relation. By default, it will also show times of implicitly missing data.\n\n\n\n\n\nGrouping the data by Id and measurement session provides easy overviews for longitudinal datasets\n\n\n\ndataset |&gt; \n  gg_overview(col = Id) + \n  ggsci::scale_color_jco() #nice color palette\n\n\n\n\n\n\n\n\n\n\ngg_heatmap()\ngg_heatmap() renders one calendar day per row within each data‑collection period. It is well‑suited to long monitoring spans and scales effectively to many participants. To highlight patterns that cross midnight, it supports a doubleplot option that displays a duplicate of the day, or the next day with an offset.\n\ndataset_red |&gt; gg_heatmap()\n\n\n\n\n\n\n\n\n\n# Looking at 5-minute bins of data\ndataset_red |&gt; gg_heatmap(unit = \"5 mins\")\n\n\n\n\n\n\n\n\n\n#showing data as doubleplots. Time breaks have to be reduced for legibility\ndataset_red |&gt; \n  gg_heatmap(doubleplot = \"next\", \n             time.breaks = c(0, 12, 24, 36, 48)*3600\n             )\n\n\n\n\n\n\n\n\n\n# Actogram-style heatmap (&lt;10 lx mel EDI in this case)\ndataset_red |&gt; \n  gg_heatmap(MEDI &lt; 10,\n             doubleplot = \"next\", \n             time.breaks = c(0, 12, 24, 36, 48)*3600,\n             fill.limits = c(0, NA), \n             fill.remove = TRUE, \n             fill.title = \"&lt;10lx mel EDI\"\n             ) +\n  scale_fill_manual(values = c(\"TRUE\" = \"black\", \"FALSE\" = \"#00000000\"))\n\n\n\n\n\n\n\n\n\n\nWhat about non-light variables?\nLightLogR is optimized for wearable light sensors and selects sensible defaults: for example, melanopic EDI (when available) and settings suited to typical light‑exposure distributions. Nevertheless, the functions are measurement‑agnostic and can be applied to non‑light variables. Consult the function documentation to see which arguments to adjust for your variable of interest. For example, here we plot an activity variable:\n\ndataset_red |&gt; \n  gg_days(\n    y.axis = PIM, #variable PIM\n    y.scale = \"identity\", #set a linear scale\n    y.axis.breaks = waiver(), #choose standard axis breaks according to values\n    y.axis.label = \"Proportional integration mode (PIM)\"\n  ) +\n  coord_cartesian(ylim = c(0, 5000))"
  },
  {
    "objectID": "beginner.html#validation",
    "href": "beginner.html#validation",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Validation",
    "text": "Validation\nCurrently, LightLogR’s validation aims to ensure a regular, uninterrupted time series for each participant. Additional features are planned.\nThe figures at the side summarize the gap terminology used in LightLogR and illustrate how gap_handler() fills implicit missing data.\n\n\n\n\n\nTerminology of gaps in LightLogR\n\n\n\n\n\ngap_handler() identifies the time series’ dominant epoch (the most common sampling interval) and fills NA entries between the first and last observation. By default, no observations are dropped, so irregular samples are preserved.\n\n\nTo quickly assess whether a dataset contains (implicit) gaps or irregular sampling, use the following diagnostic helpers:\n\ndataset |&gt; has_gaps()\n\n[1] TRUE\n\ndataset |&gt; has_irregulars()\n\n[1] TRUE\n\n\nWe can then quickly visualize where these issues occur within the affected days.\n\ndataset |&gt; gg_gaps(group.by.days = TRUE, show.irregulars = TRUE)\n\n\n\n\n\n\n\n\nThis function can be slow when a dataset contains many gaps or irregular samples. If needed, pre‑filter the data or adjust the function’s arguments.\nIn our example, we identify eight participant‑days with gaps:\n\nThree straightforward cases: data collection ends around noon on Monday, leaving the remainder of the day missing. By default, the function evaluates complete calendar days (this is configurable). These days only require converting implicit gaps into explicit missing values.\nTwo pre‑trial snippets: brief measurements occur on the Friday or Monday preceding the trial—likely test recordings. These days are outside the study window and should be removed entirely.\nThree early irregularities: irregular sampling appears shortly after data collection starts. This most likely reflects a test recording immediately before the device was handed to the participant. Trimming this initial segment eliminates the irregularity and the rest of the day can be changed to explicit missingness.\n\n\nPreparing the dataset\nThere are several ways to address these issues. We will showcase three in the next sections.\n\n1. Set the maximum length of the dataset.\nIf the study follows a fixed‑length protocol, you can enforce a maximum observation window (e.g., 7 days) by trimming from the beginning so that each participant’s series has the same duration. This approach preserves participant‑specific end times, which must meaningfully reflect protocol completion; otherwise, you risk cutting away valid data.\n\ndataset |&gt; \nfilter(\n  Datetime &gt; (max(Datetime) - days(7))\n  ) |&gt; \n  gg_gaps(group.by.days = TRUE, show.irregulars = TRUE)\n\n\n\n\n\n\n\n\nThe remaining gaps are simple start‑ and end‑day truncations.\n\n\n2. Remove the first values from the dataset\nYou can remove a fixed number of observations from the beginning of each participant’s series. This approach is helpful when the exact total measurement duration is not critical—for example, to discard brief pre‑trial test recordings or initial device‑stabilization periods.\n\ndataset |&gt; \n  slice_tail(n = -(3*60*6)) |&gt; \n  gg_gaps(group.by.days = TRUE, show.irregulars = TRUE)\n\n\n\n\n\n\n\n\nThe results are similarly effective.\n\n\n3. Trim with a list\nThe most robust way to enforce sensible measurement windows is to supply a table of trial start and end timestamps (per participant) and filter the time series accordingly. In this tutorial we create that table on the fly; in practice, it is typically stored in a CSV or Excel file. The add_states() function provides an effective interface between the two datasets: it aligns by identifier and time, adds state information (e.g., “in‑trial”), and enables precise trimming. Ensure that the identifying variables (e.g., Id) are named identically across files.\n\n#create a dataframe of trial times\ntrial_times &lt;-\n  data.frame(\n    Id = c(\"216\", \"218\", \"219\"),\n    start = c(\n      \"02.10.2023  12:30:00\",\n      \"16.10.2023  12:00:00\",\n      \"16.10.2023  12:00:00\"\n    ),\n    end = c(\n      \"09.10.2023  12:30:00\",\n      \"23.10.2023  12:00:00\",\n      \"23.10.2023  12:00:00\"\n    ),\n    trial = TRUE\n  ) |&gt;\n  mutate(across(\n    c(start, end),\n    \\(x) parse_date_time(x, order = \"%d%m%y %H%M%S\", tz = \"Europe/Berlin\")\n  )) |&gt;\n  group_by(Id)\n\n\n# filter dataset by trial time\ndataset &lt;-\n  dataset |&gt;\n  add_states(trial_times) |&gt;\n  dplyr::filter(trial) |&gt;\n  select(-trial)\n\ndataset |&gt; \n  gg_gaps(group.by.days = TRUE, show.irregulars = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ngap_table()\nWe can summarize each dataset’s regularity and missingness in a table. Note that this function may be slow when many gaps are present.\n\ndataset |&gt; gap_table() |&gt; cols_hide(contains(\"_n\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of available and missing data\n\n\nVariable: melanopic EDI\n\n\n\n\nData\n\n\nMissing\n\n\n\n\n\nRegular\n\n\nIrregular\n\n\nRange\n\n\nInterval\n\n\nGaps\n\n\nImplicit\n\n\nExplicit\n\n\n\nTime\n%\nn1,2\nTime\nTime\nN\nø\nTime\n%\nTime\n%\nTime\n%\n\n\n\n\nOverall\n2w 6d 21h 28m 10s\n87.1%3\n0\n3w 3d\n10\n6\n1d 13h 15m 55s\n3d 2h 31m 50s\n12.9%3\n3d 2h 31m 50s\n12.9%3\n0s\n0.0%3\n\n\n216\n\n\n\n1w\n87.5%\n0\n1w 1d\n10s\n2\n12h\n1d\n12.5%\n1d\n12.5%\n0s\n0.0%\n\n\n218\n\n\n\n6d 21h 58m 50s\n86.4%\n0\n1w 1d\n10s\n2\n13h 35s\n1d 2h 1m 10s\n13.6%\n1d 2h 1m 10s\n13.6%\n0s\n0.0%\n\n\n219\n\n\n\n6d 23h 29m 20s\n87.2%\n0\n1w 1d\n10s\n2\n12h 15m 20s\n1d 30m 40s\n12.8%\n1d 30m 40s\n12.8%\n0s\n0.0%\n\n\n\n1 If n &gt; 0: it is possible that the other summary statistics are affected, as they are calculated based on the most prominent interval.\n\n\n2 Number of (missing or actual) observations\n\n\n3 Based on times, not necessarily number of observations\n\n\n\n\n\n\n\n\n\n\ngap_handler()\nApproximately 13% of the missing data are implicit—they arise from truncated start and end days. It is good practice to make these gaps explicit. Use gap_handler(full.days = TRUE) to fill implicit gaps to full‑day regularity. Then verify the result with gap_table(), the diagnostic helpers, and a follow‑up visualization:\n\ndataset &lt;- dataset |&gt; gap_handler(full.days = TRUE)\ndataset |&gt; gap_table() |&gt; cols_hide(contains(\"_n\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of available and missing data\n\n\nVariable: melanopic EDI\n\n\n\n\nData\n\n\nMissing\n\n\n\n\n\nRegular\n\n\nIrregular\n\n\nRange\n\n\nInterval\n\n\nGaps\n\n\nImplicit\n\n\nExplicit\n\n\n\nTime\n%\nn1,2\nTime\nTime\nN\nø\nTime\n%\nTime\n%\nTime\n%\n\n\n\n\nOverall\n2w 6d 21h 28m 10s\n87.1%3\n0\n3w 3d\n10\n6\n1d 13h 15m 55s\n3d 2h 31m 50s\n12.9%3\n0s\n0.0%3\n3d 2h 31m 50s\n12.9%3\n\n\n216\n\n\n\n1w\n87.5%\n0\n1w 1d\n10s\n2\n12h\n1d\n12.5%\n0s\n0.0%\n1d\n12.5%\n\n\n218\n\n\n\n6d 21h 58m 50s\n86.4%\n0\n1w 1d\n10s\n2\n13h 35s\n1d 2h 1m 10s\n13.6%\n0s\n0.0%\n1d 2h 1m 10s\n13.6%\n\n\n219\n\n\n\n6d 23h 29m 20s\n87.2%\n0\n1w 1d\n10s\n2\n12h 15m 20s\n1d 30m 40s\n12.8%\n0s\n0.0%\n1d 30m 40s\n12.8%\n\n\n\n1 If n &gt; 0: it is possible that the other summary statistics are affected, as they are calculated based on the most prominent interval.\n\n\n2 Number of (missing or actual) observations\n\n\n3 Based on times, not necessarily number of observations\n\n\n\n\n\n\n\n\n\ndataset |&gt; has_gaps()\n\n[1] FALSE\n\ndataset |&gt; has_irregulars()\n\n[1] FALSE\n\n\n\ndataset |&gt; gg_days(aes_col = Id)\n\n\n\n\n\n\n\n\n\n\nremove_partial_data()\nIt is often necessary to set missingness thresholds at different levels (hour, day, participant). Typical questions include:\n\nHow much data may be missing within an hour before that hour is excluded?\nHow much data may be missing from a day before that day is excluded?\nHow much data may be missing for a participant before excluding them from further analyses?\n\nremove_partial_data() addresses these questions. It evaluates each group (by default, Id) and quantifies missingness either as an absolute duration or a relative proportion. Groups that exceed the specified threshold are discarded. A useful option is by.date, which performs the thresholding per calendar day (for removal) while leaving the output grouping unchanged. Note that missingness is determined by the amount of data points in each group, relative to NA values.\nFor this tutorial, we will remove any day with more than one hour of missing data—this effectively drops both partial Mondays:\n\ndataset &lt;- \n  dataset |&gt; \n  remove_partial_data(Variable.colname = MEDI, \n                      threshold.missing = \"1 hour\",\n                      by.date = TRUE)\n\ndataset |&gt; gg_days(aes_col = Id)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy did we just spend all this time handling gaps and irregularities on the Mondays only to remove them afterward?\nNot all datasets are this straightforward. Deciding whether a day should be included in the analysis should come after ensuring the data are aligned to a regular, uninterrupted time series. Regularization makes diagnostics meaningful and prevents threshold rules from behaving unpredictably.\nMoreover, there are different frameworks for grouping personal light‑exposure data. In this tutorial we focus on calendar dates and 24‑hour days. Other frameworks group differently. For example, anchoring to sleep–wake cycles—under which both Mondays might still contain useful nocturnal data. Harmonizing first ensures those alternatives remain viable even if calendar‑day summaries are later excluded."
  },
  {
    "objectID": "beginner.html#metrics",
    "href": "beginner.html#metrics",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Metrics",
    "text": "Metrics\nMetrics form the second major pillar of LightLogR, alongside visualization. The literature contains many light‑exposure metrics; LightLogR implements a broad set of them behind a uniform, well‑documented interface. The currently available metrics are:\n\n\n\n\n\n\n\n\n\nMetric Family\nSubmetrics\nNote\nDocumentation\n\n\n\n\nBarroso\n7\n\nbarroso_lighting_metrics()\n\n\nBright-dark period\n4x2\nbright / dark\nbright_dark_period()\n\n\nCentroid of light exposure\n1\n\ncentroidLE()\n\n\nDose\n1\n\ndose()\n\n\nDisparity index\n1\n\ndisparity_index()\n\n\nDuration above threshold\n3\nabove, below, within\nduration_above_threshold()\n\n\nExponential moving average (EMA)\n1\n\nexponential_moving_average()\n\n\nFrequency crossing threshold\n1\n\nfrequency_crossing_threshold()\n\n\nIntradaily Variance (IV)\n1\n\nintradaily_variability()\n\n\nInterdaily Stability (IS)\n1\n\ninterdaily_stability()\n\n\nMidpoint CE (Cumulative Exposure)\n1\n\nmidpointCE()\n\n\nnvRC (Non-visual circadian response)\n4\n\nnvRC(), nvRC_circadianDisturbance(), nvRC_circadianBias(), nvRC_relativeAmplitudeError()\n\n\nnvRD (Non-visual direct response)\n2\n\nnvRD(), nvRD_cumulative_response()\n\n\nPeriod above threshold\n3\nabove, below, within\nperiod_above_threshold()\n\n\nPulses above threshold\n7x3\nabove, below, within\npulses_above_threshold()\n\n\nThreshold for duration\n2\nabove, below\nthreshold_for_duration()\n\n\nTiming above threshold\n3\nabove, below, within\ntiming_above_threshold()\n\n\nTotal:\n\n\n\n\n\n17 families\n62 metrics\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLightLogR supports a wide range of metrics across different metric families. You can find the full documentation of metrics functions in the reference section. There is also an overview article on how to use Metrics.\nIf you would like to use a metric you don’t find represented in LightLogR, please contact the developers. The easiest and most trackable way to get in contact is by opening a new issue on our Github repository.\n\n\n\nPrinciples\nEach metric function operates on vectors. Although the main argument is often named Light.vector, the name is conventional - the function will accept any variable you supply. All metric functions are thoroughly documented, with references to their intended use and interpretation.\nWhile we don’t generally recommend it, you can pass a raw vector directly to a metric function. For example, to compute Time above 250 lx melanopic EDI, you could run:\n\nduration_above_threshold(\n  Light.vector = dataset$MEDI,\n  Time.vector = dataset$Datetime,\n  threshold = 250\n)\n\n[1] \"179840s (~2.08 days)\"\n\n\nHowever, that single result is not very informative - it aggregates across all participants and all days. To recover the total recorded duration, recompute the complementary metric: Time below 250 lx melanopic EDI. This should approximate the full two weeks and four days of data when evaluated over the whole dataset:\n\nduration_above_threshold(\n  Light.vector = dataset$MEDI,\n  Time.vector = dataset$Datetime,\n  threshold = 250,\n  comparison = \"below\"\n)\n\n[1] \"1375360s (~2.27 weeks)\"\n\n\nThe problem is amplified for metrics defined at the day scale (or shorter). For example, the brightest 10 hours (M10) is computed within each 24‑hour day using a consecutive 10‑hour window—so applying it to a pooled, cross‑day vector is almost meaningless:\n\nbright_dark_period(\n  Light.vector = dataset$MEDI,\n  Time.vector = dataset$Datetime,\n  as.df = TRUE\n) |&gt; \n  gt() |&gt; tab_header(\"M10\")\n\nWarning in bright_dark_period(Light.vector = dataset$MEDI, Time.vector =\ndataset$Datetime, : `Time.vector` is not regularly spaced. Calculated results\nmay be incorrect!\n\n\n\n\n\n\n\n\nM10\n\n\nbrightest_10h_mean\nbrightest_10h_midpoint\nbrightest_10h_onset\nbrightest_10h_offset\n\n\n\n\n3292.002\n2023-10-08 15:17:49\n2023-10-08 10:17:59\n2023-10-08 20:17:49\n\n\n\n\n\n\n\nThe resulting value - although computationally valid - is substantively meaningless: it selects the single brightest 10‑hour window across all participants, rather than computing M10 per participant per day. In addition, two time series (218 & 219) overlap in time, which violates the assumption of a single, regularly spaced series and can produce errors. Hence the Warning: Time.vector is not regularly spaced. Calculated results may be incorrect!\nAccordingly, metric functions should be applied within tidy groups (e.g., by Id and by calendar Date), not to a pooled vector. You can achieve this with explicit for‑loops or, preferably, a tidy approach using dplyr (e.g., group_by()/summarise() or nest()/map()). We recommend the latter.\n\n\nUse of summarize()\nWrap the metric inside a dplyr summarise()/summarize() call, supply the grouped dataset, and set as.df = TRUE. This yields a tidy, one‑row‑per‑group result (e.g., per Id). For example, computing interdaily stability (IS):\n\ndataset |&gt; \n  summarize(\n    interdaily_stability(\n      Light.vector = MEDI,\n      Datetime.vector = Datetime,\n      as.df = TRUE\n    )\n  )\n\n# A tibble: 3 × 2\n  Id    interdaily_stability\n  &lt;chr&gt;                &lt;dbl&gt;\n1 216                  0.260\n2 218                  0.339\n3 219                  0.389\n\n\nTo compute multiple metrics at once, include additional expressions inside the summarize() call. For instance, add Time above 250 lx melanopic EDI alongside IS:\n\ndataset |&gt; \n  summarize(\n     duration_above_threshold(\n      Light.vector = MEDI,\n      Time.vector = Datetime,\n      threshold = 250,\n      as.df = TRUE\n    ),\n    interdaily_stability(\n      Light.vector = MEDI,\n      Datetime.vector = Datetime,\n      as.df = TRUE\n    )\n  )\n\n# A tibble: 3 × 3\n  Id    duration_above_250    interdaily_stability\n  &lt;chr&gt; &lt;Duration&gt;                           &lt;dbl&gt;\n1 216   89160s (~1.03 days)                  0.260\n2 218   31160s (~8.66 hours)                 0.339\n3 219   59520s (~16.53 hours)                0.389\n\n\nFor finer granularity, add additional grouping variables before summarizing—for example, group by calendar Date to compute metrics per participant–day:\n\nTAT250 &lt;- \ndataset |&gt; \n  add_Date_col(group.by = TRUE, as.wday = TRUE) |&gt; #add a Date column + group\n    summarize(\n     duration_above_threshold(\n      Light.vector = MEDI,\n      Time.vector = Datetime,\n      threshold = 250,\n      as.df = TRUE\n    ),\n    .groups = \"drop_last\"\n  )\n\nTAT250 |&gt; gt()\n\n\n\n\n\n\n\nDate\nduration_above_250\n\n\n\n\n216\n\n\nTue\n15360s (~4.27 hours)\n\n\nWed\n14910s (~4.14 hours)\n\n\nThu\n17550s (~4.88 hours)\n\n\nFri\n9900s (~2.75 hours)\n\n\nSat\n12180s (~3.38 hours)\n\n\nSun\n19260s (~5.35 hours)\n\n\n218\n\n\nTue\n1900s (~31.67 minutes)\n\n\nWed\n1080s (~18 minutes)\n\n\nThu\n760s (~12.67 minutes)\n\n\nFri\n6170s (~1.71 hours)\n\n\nSat\n10370s (~2.88 hours)\n\n\nSun\n10880s (~3.02 hours)\n\n\n219\n\n\nTue\n12250s (~3.4 hours)\n\n\nWed\n14400s (~4 hours)\n\n\nThu\n10200s (~2.83 hours)\n\n\nFri\n8940s (~2.48 hours)\n\n\nSat\n5820s (~1.62 hours)\n\n\nSun\n7910s (~2.2 hours)\n\n\n\n\n\n\n\nWe can further condense this:\n\nTAT250 |&gt; \n  summarize_numeric() |&gt; \n  gt()\n\n\n\n\n\n\n\nId\nmean_duration_above_250\nepisodes\n\n\n\n\n216\n14860s (~4.13 hours)\n6\n\n\n218\n5193s (~1.44 hours)\n6\n\n\n219\n9920s (~2.76 hours)\n6\n\n\n\n\n\n\n\nThat’s all you need to get started with metric calculation in LightLogR. While advanced metrics involve additional considerations, this tidy grouped workflow will take you a long way."
  },
  {
    "objectID": "beginner.html#photoperiod",
    "href": "beginner.html#photoperiod",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Photoperiod",
    "text": "Photoperiod\nPhotoperiod is a key covariate in many analyses of personal light exposure. LightLogR includes utilities to derive photoperiod information with minimal effort. All you need are geographic coordinates in decimal degrees (latitude, longitude); functions will align photoperiod features to your time series. Provide coordinates in standard decimal format (e.g., 48.52, 9.06):\n\n#specifying coordinates (latitude/longitude)\ncoordinates &lt;- c(48.521637, 9.057645)\n\n#extracting photoperiod information\ndataset |&gt; extract_photoperiod(coordinates)\n\n         date            tz      lat      lon solar.angle                dawn\n1  2023-10-03 Europe/Berlin 48.52164 9.057645          -6 2023-10-03 06:54:12\n2  2023-10-04 Europe/Berlin 48.52164 9.057645          -6 2023-10-04 06:55:38\n3  2023-10-05 Europe/Berlin 48.52164 9.057645          -6 2023-10-05 06:57:04\n4  2023-10-06 Europe/Berlin 48.52164 9.057645          -6 2023-10-06 06:58:30\n5  2023-10-07 Europe/Berlin 48.52164 9.057645          -6 2023-10-07 06:59:56\n6  2023-10-08 Europe/Berlin 48.52164 9.057645          -6 2023-10-08 07:01:22\n7  2023-10-17 Europe/Berlin 48.52164 9.057645          -6 2023-10-17 07:14:23\n8  2023-10-18 Europe/Berlin 48.52164 9.057645          -6 2023-10-18 07:15:50\n9  2023-10-19 Europe/Berlin 48.52164 9.057645          -6 2023-10-19 07:17:17\n10 2023-10-20 Europe/Berlin 48.52164 9.057645          -6 2023-10-20 07:18:45\n11 2023-10-21 Europe/Berlin 48.52164 9.057645          -6 2023-10-21 07:20:13\n12 2023-10-22 Europe/Berlin 48.52164 9.057645          -6 2023-10-22 07:21:40\n                  dusk    photoperiod\n1  2023-10-03 19:30:44 12.60884 hours\n2  2023-10-04 19:28:41 12.55080 hours\n3  2023-10-05 19:26:39 12.49293 hours\n4  2023-10-06 19:24:37 12.43521 hours\n5  2023-10-07 19:22:36 12.37767 hours\n6  2023-10-08 19:20:35 12.32031 hours\n7  2023-10-17 19:03:11 11.81335 hours\n8  2023-10-18 19:01:19 11.75821 hours\n9  2023-10-19 18:59:29 11.70333 hours\n10 2023-10-20 18:57:40 11.64874 hours\n11 2023-10-21 18:55:53 11.59443 hours\n12 2023-10-22 18:54:06 11.54043 hours\n\n\n\n#adding photoperiod information\ndataset &lt;- \n  dataset |&gt; \n  add_photoperiod(coordinates)\n\ndataset |&gt; head()\n\n# A tibble: 6 × 9\n# Groups:   Id [1]\n  Id    Datetime            is.implicit   PIM  MEDI dawn               \n  &lt;chr&gt; &lt;dttm&gt;              &lt;lgl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;             \n1 216   2023-10-03 00:00:09 FALSE          91  12.6 2023-10-03 06:54:12\n2 216   2023-10-03 00:00:19 FALSE          62  14.3 2023-10-03 06:54:12\n3 216   2023-10-03 00:00:29 FALSE          45  10.2 2023-10-03 06:54:12\n4 216   2023-10-03 00:00:39 FALSE          13  10.3 2023-10-03 06:54:12\n5 216   2023-10-03 00:00:49 FALSE         216  11.9 2023-10-03 06:54:12\n6 216   2023-10-03 00:00:59 FALSE          62  11.6 2023-10-03 06:54:12\n# ℹ 3 more variables: dusk &lt;dttm&gt;, photoperiod &lt;drtn&gt;, photoperiod.state &lt;chr&gt;\n\n\n\nPhotoperiod in visualizations\n\n#if photoperiod information was already added to the data\n#nothing has to be specified\ndataset |&gt; gg_days() |&gt; gg_photoperiod()\n\n\n\n\n\n\n\n\n\n#if no photoperiod information is available in the data, coordinates have to\n#be specified\ndataset_red |&gt; gg_days() |&gt; gg_photoperiod(coordinates)\n\n\n\n\n\n\n\n\n\n\nData\nPhotoperiod features make it easy to split data into day and night states—for example, to compute metrics by phase. The number_states() function places a counter each time the state changes, effectively numbering successive day and night episodes. Grouping by these counters then allows you to calculate metrics for individual days and nights:\n\ndataset |&gt; \n  #create numbered days and nights:\n  number_states(photoperiod.state) |&gt; \n  #group by Id, day and nights, and also the numbers:\n  group_by(photoperiod.state, photoperiod.state.count, .add = TRUE) |&gt; \n  #calculate the brightest hour in each day and each night:\n  summarize(\n    bright_dark_period(MEDI, Datetime, timespan = \"1 hour\", as.df = TRUE),\n    .groups = \"drop_last\") |&gt; \n  #select (bright_dark_period calulates four metrics: start, end, middle, mean)\n  select(Id, photoperiod.state, brightest_1h_mean) |&gt;\n  #condense the instances to a single summary\n  summarize_numeric(prefix = \"\") |&gt; \n  #show as table\n  gt() |&gt; fmt_number()\n\n\n\n\n\n\n\nphotoperiod.state\nbrightest_1h_mean\nepisodes\n\n\n\n\n216\n\n\nday\n7,294.36\n6.00\n\n\nnight\n34.87\n7.00\n\n\n218\n\n\nday\n1,246.30\n6.00\n\n\nnight\n72.96\n7.00\n\n\n219\n\n\nday\n1,670.66\n6.00\n\n\nnight\n51.40\n7.00\n\n\n\n\n\n\n\nThis yields the average brightest 1‑hour period for each participant, separately for day and night. Notably, the participant with the highest daytime brightness also shows the lowest nighttime brightness, and vice versa."
  },
  {
    "objectID": "beginner.html#distribution-of-light-exposure",
    "href": "beginner.html#distribution-of-light-exposure",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Distribution of light exposure",
    "text": "Distribution of light exposure\nPersonal light‑exposure data exhibit a characteristic distribution (see figure): they are strongly right‑skewed—approximately log‑normal—and contain many zeros (i.e., zero‑inflation).\n\n\n\n\n\nDistribution of light exposure in the environment and for a participant, both at night and day\n\n\nConsequently, the arithmetic mean is not a representative summary for these data. We can visualize this by placing common location metrics on the distribution.\n\ndataset |&gt; \n  ungroup() |&gt; \n  summarize(\n    mean = mean(MEDI),\n    median = median(MEDI),\n    geo_mean = exp(mean(log(MEDI[MEDI &gt; 0]), na.rm = TRUE))\n  ) |&gt; \n  gt()\n\n\n\n\n\n\n\nmean\nmedian\ngeo_mean\n\n\n\n\n282.535\n8.76\n32.82594\n\n\n\n\n\n\n\n\ndataset |&gt; \n  aggregate_Datetime(\"5 min\") |&gt; \n  ggplot(aes(x=MEDI, y = after_stat(ncount))) +\n  geom_histogram(binwidth = 0.2) +\n  scale_x_continuous(trans = \"symlog\", \n                     breaks = c(0, 10^(0:5)), \n                     labels= expression(0,10^0,10^1, 10^2, 10^3, 10^4, 10^5)\n                     ) +\n  geom_vline(xintercept = c(282, 9, 33), col = \"red\") +\n  theme_minimal() +\n  # facet_wrap(~Id) +\n  labs(x = \"Melanopic illuminance (lx, mel EDI)\", y = \"Scaled counts (max = 1)\")\n\n\n\n\n\n\n\n\nTo better characterize zero‑inflated, right‑skewed light data, use log_zero_inflated(). The function adds a small constant (ε) to every observation before taking logs, making the transform well‑defined at zero. Choose ε based on the device’s measurement resolution/accuracy; for wearables spanning roughly 1–10^5 lx, we recommend ε = 0.1 lx. The inverse, exp_zero_inflated(), returns values to the original scale by exponentiating and then subtracting the same ε. The default basis for these functions is 10.\n\ndataset |&gt; \n  ungroup() |&gt; \n  summarize(\n    mean = mean(MEDI),\n    median = median(MEDI),\n    geo_mean =  exp(mean(log(MEDI[MEDI &gt; 0]), na.rm = TRUE)),\n    log_zero_inflated_mean = \n      MEDI |&gt; log_zero_inflated() |&gt; mean() |&gt; exp_zero_inflated()\n  ) |&gt; \n  gt()\n\n\n\n\n\n\n\nmean\nmedian\ngeo_mean\nlog_zero_inflated_mean\n\n\n\n\n282.535\n8.76\n32.82594\n6.567388\n\n\n\n\n\n\n\n\ndataset |&gt; \n  aggregate_Datetime(\"5 min\") |&gt; \n  ggplot(aes(x=MEDI, y = after_stat(ncount))) +\n  geom_histogram(binwidth = 0.2) +\n  scale_x_continuous(trans = \"symlog\", \n                     breaks = c(0, 10^(0:5)), \n                     labels= expression(0,10^0,10^1, 10^2, 10^3, 10^4, 10^5)\n                     ) +\n  geom_vline(xintercept = c(282, 9, 33, 7), col = \"red\") +\n  theme_minimal() +\n  # facet_wrap(~Id) +\n  labs(x = \"Melanopic illuminance (lx, mel EDI)\", y = \"Scaled counts (max = 1)\")\n\n\n\n\n\n\n\n\n\nLog zero-inflated with metrics\nWhen computing averaging metrics, apply the transformation explicitly to the variable you pass to the metric. This ensures the statistic is computed on the intended scale and makes your code easy to audit later.\nFor the zero‑inflated log approach, transform before averaging and (if desired) back‑transform for reporting:\n\ndataset |&gt; \n  filter(Id == \"216\") |&gt; \n  add_Date_col(group.by = TRUE) |&gt; \n  summarize(\n    #without transformation:\n    bright_dark_period(MEDI, Datetime, as.df = TRUE),\n    #with transformation:\n    bright_dark_period(\n      log_zero_inflated(MEDI),\n      timespan = \"9.99 hours\", #needs to be different from 10 or it overwrites\n      Datetime, as.df = TRUE),\n    .groups = \"drop_last\"\n            ) |&gt; \n  select(Id, Date, brightest_10h_mean, brightest_9.99h_mean) |&gt; \n  mutate(brightest_9.99h_mean = exp_zero_inflated(brightest_9.99h_mean)) |&gt; \n  rename(brightest_10h_zero_infl_mean = brightest_9.99h_mean) |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n\n\nDate\nbrightest_10h_mean\nbrightest_10h_zero_infl_mean\n\n\n\n\n216\n\n\n2023-10-03\n774.39\n164.46\n\n\n2023-10-04\n1,890.69\n106.63\n\n\n2023-10-05\n355.81\n153.26\n\n\n2023-10-06\n706.41\n117.82\n\n\n2023-10-07\n790.83\n125.38\n\n\n2023-10-08\n3,292.00\n250.35"
  },
  {
    "objectID": "beginner.html#summaries",
    "href": "beginner.html#summaries",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Summaries",
    "text": "Summaries\nSummary helpers provide fast, dataset‑wide overviews. Existing examples include gap_table() (tabular diagnostics) and gg_overview() (visual timeline). In the next release, two higher‑level tools are planned: grand_overview() (a dataset‑level summary plot) and light_summary_table() (a table of key exposure metrics). These are not available in LightLogR 0.9.3 but are slated for an upcoming release and are shown in the next two sections as sneak previews. In keeping with LightLogR’s design, they will have straightforward interfaces and play well with grouped/tidy workflows.\n\nSummary plot\n\ndataset |&gt; \n  grand_overview(coordinates, #provide the coordinates\n                 \"Tübingen\", #provide a site name\n                 \"Germany\", #provide a country name\n                 \"#DDCC77\", #provide a color for the dataset\n                 photoperiod_sequence = 1 #specify the photoperiod resolution\n                 )\n\n\n\n\n\n\n\n# ggsave(\"assets/grand_overview.png\", width = 17, height = 10, scale = 2, units = \"cm\")\n\n\n\nSummary table\n\nsummary_table &lt;- \ndataset |&gt; \nlight_summary_table(\n  coordinates, #provide coordinates\n  \"Tuebingen\", #provide a site name\n  \"Germany\", #provide a country name\n  \"#DDCC77\", #provide a color for histograms\n  histograms = TRUE #show histograms\n)\nsummary_table \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary table\n\n\nTuebingen, Germany (48.5°N, 9.1°E), TZ: Europe/Berlin\n\n\n\n\nOverview\n\n\nParticipants\n\n\n3\n\n\n\n\n\n\nParticipant-days\n\n\n18\n\n\n\n\n\n\nDays ≥80% complete\n\n\n18\n\n\n\n\n\n\nMissing/irregular\n\n\n0.00% (0.00% - 0.00%)\n\n\n\n   \n\n\n\n\nPhotoperiod\n\n\n11h 56m (11h 32m - 12h 36m)\n\n\n\n   \n\n1 \n\n\nMetrics2\n\n\nDose\nD (lx·h)\n6,781 ±7,698 (1,226 - 33,160)\n\n\n\n   \n\n\n\n\nDuration above 250 lx\nTAT250\n2h 46m ±1h 29m (12m 40s - 5h 21m)\n\n\n\n   \n\n\n\n\nDuration within 1-10 lx\nTWT1–10\n4h 51m ±2h 5m (1h 39m - 8h 55m)\n\n\n\n   \n\n\n\n\nDuration below 1 lx\nTBT1\n7h 34m ±2h 39m (12m 50s - 11h 41m)\n\n\n\n   \n\n\n\n\nPeriod above 250 lx\nPAT250\n32m 53s ±24m 1s (2m 50s - 1h 22m)\n\n\n\n   \n\n\n\n\nDuration above 1000 lx\nTAT1000\n1h 4m ±55m 48s (7m 30s - 3h 4m)\n\n\n\n   \n\n\n\n\nFirst timing above 250 lx\nFLiT250\n10:55 ±01:26 (08:31 - 13:41)\n\n\n\n   \n\n1 \n\n\nMean timing above 250 lx\nMLiT250\n14:11 ±01:30 (10:44 - 16:23)\n\n\n\n   \n\n1 \n\n\nLast timing above 250 lx\nLLiT250\n18:40 ±02:09 (13:56 - 23:03)\n\n\n\n   \n\n1 \n\n\nBrightest 10h midpoint\nM10midpoint\n14:47 ±01:15 (13:10 - 17:21)\n\n\n\n   \n\n1 \n\n\nDarkest 5h midpoint\nL5midpoint\n03:53 ±00:53 (02:29 - 05:58)\n\n\n\n   \n\n1 \n\n\nBrightest 10h mean3\nM10mean (lx)\n126.6 ±63.3 (17.6 - 249.3)\n\n\n\n   \n\n\n\n\nDarkest 5h mean3\nL5mean (lx)\n0.1 ±0.5 (0.0 - 2.1)\n\n\n\n   \n\n\n\n\nInterdaily stability\nIS\n0.329 ±0.065 (0.260 - 0.389)\n\n\n\n   \n\n\n\n\nIntradaily variability\nIV\n1.206 ±0.175 (1.013 - 1.353)\n\n\n\n   \n\n\n\n\n\nvalues show: mean ±sd (min - max) and are all based on measurements of melanopic EDI (lx)\n\n\n1 Histogram limits are set from 00:00 to 24:00\n\n\n2 Metrics are calculated on a by-participant-day basis (n=18) with the exception of IV and IS, which are calculated on a by-participant basis (n=3).\n\n\n3 Values were log 10 transformed prior to averaging, with an offset of 0.1, and backtransformed afterwards\n\n\n\n\n\n\n\n# summary_table |&gt; gtsave(\"assets/table_summary.png\", vwidth = 820)"
  },
  {
    "objectID": "beginner.html#processing-states",
    "href": "beginner.html#processing-states",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Processing & states",
    "text": "Processing & states\nLightLogR contains many functions to manipulate, expand, or condense datasets. We will highlight the most important ones.\n\naggregate_Datetime()\naggregate_Datetime() is a general‑purpose resampling utility that bins observations into fixed‑duration intervals and computes a summary statistic per bin. It is intentionally opinionated, providing sensible defaults (e.g., mean for numeric columns and mode for character/factor columns), but all summaries are configurable and additional metrics can be requested. Use it as a lightweight formatter to change the effective measurement interval after the fact (e.g., re‑epoching from 10 s to 1 min).\n\n  dataset |&gt; \n  aggregate_Datetime(\"1 hour\") |&gt; #try to set different units: \"15 mins\", \"2 hours\",...\n  gg_days(aes_col = Id)\n\n\n\n\n\n\n\n\n\n\naggregate_Date()\naggregate_Date() is a companion function that collapses each group into a single 24‑hour profile, optionally re‑epoching the data in the process. It is well‑suited to very large datasets when you need an overview of the average day. It applies the same summarization rules as aggregate_Datetime() and is equally configurable to your requirements:\n\ndataset |&gt; \n  aggregate_Date(unit = \"5 minutes\") |&gt; \n  gg_days(aes_col = Id)\n\n\n\n\n\n\n\n\n\n\ngg_doubleplot()\naggregate_Date() pairs well with gg_doubleplot(), which duplicates each day with an offset to reveal patterns that span midnight. While it can be applied to any dataset, use it on only a handful of days at a time to keep plots readable. If the dataset it is called on contains more than one day gg_doubleplot() defaults to displaying the next day instead of duplicating the same day.\n\ndataset |&gt; \n  aggregate_Date(unit = \"30 minutes\") |&gt; \n  gg_doubleplot(aes_col = Id, aes_fill = Id)\n\n\n\n\n\n\n\n\n\n# it is recommended to add photoperiod information after aggregating to the Date\n# level and prior to the doubleplot for best results.\ndataset |&gt; \n  aggregate_Date(unit = \"30 minutes\") |&gt; \n  add_photoperiod(coordinates, overwrite = TRUE) |&gt;\n  gg_doubleplot(aes_fill = Id) |&gt;\n  gg_photoperiod()\n\n\n\n\n\n\n\n\n\n\nBeyond inital variables\nBoth aggregate_Datetime() and aggregate_Date() allow for the calculation of additional metrics within their respective bins. One use case is to gauge the spread of the data within certain times. A simple approach is to plot the minimum and maximum value of a dataset that was condensed to a single day.\n\ndataset |&gt; \n  aggregate_Date(unit = \"30 minutes\",\n                 lower = min(MEDI), #create new variables...\n                 upper = max(MEDI)  #...as many as needed\n                 ) |&gt; \n  gg_doubleplot(geom = \"blank\") + # use gg_doubleplot only as a canvas\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = Id), alpha = 0.5) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\nStates\nStates, in the context of LightLogR, means any non-numeric variable. Those can be part of the dataset, be calculated from the dataset (e.g., mel EDI &gt;= 250 lx), or added from an external source. We showcase some capabilities by dividing the dataset into sections by the Brown et al. (2022) recommendations for healthy lighting, using the Brown_cut() function.\n\ndataset_1day &lt;- \ndataset |&gt; \n  Brown_cut() |&gt; #creating a column with the cuts\n  aggregate_Date(unit = \"30 minutes\", \n                 numeric.handler = median #note that we switched from mean to median\n                 ) |&gt; \n  mutate(state = state |&gt; fct_relevel(\"≥250lx\", \"≤10lx\", \"≤1lx\")) #order the levels\n\n\ngg_state()\ngg_state() augments an existing plot by adding background rectangles that mark state intervals. When multiple states are present, map them to distinct fills (or colors) to improve readability.\n\ndataset_1day |&gt; \n  gg_doubleplot(col = \"black\", alpha = 1, geom = \"line\") |&gt;\n  gg_state(State.colname = state, aes_fill = state) +\n  labs(fill = \"Brown levels\")\n\n\n\n\n\n\n\n\n\n\n\ndurations()\nIf you need a numeric summary of states, durations() computes the total time spent in each state per grouping (e.g., by Id, by day). With a small reshaping step, you can produce a tidy table showing the average duration each participant spends in each state:\n\ndataset_1day |&gt; \n  group_by(state, .add = TRUE) |&gt; #adding Brown states to the grouping\n  durations(MEDI) |&gt; #calculating durations\n  ungroup() |&gt;  #remove all grouping\n  mutate(state = fct_na_value_to_level(state, \"10-250lx\")) |&gt; #name NA level\n  pivot_wider(id_cols = Id, names_from = state, values_from = duration) |&gt; #reshape\n  gt()\n\n\n\n\n\n\n\nId\n≥250lx\n≤10lx\n≤1lx\n10-250lx\n\n\n\n\n216\n21600s (~6 hours)\n14400s (~4 hours)\n23400s (~6.5 hours)\n27000s (~7.5 hours)\n\n\n218\n10800s (~3 hours)\n5400s (~1.5 hours)\n28800s (~8 hours)\n48600s (~13.5 hours)\n\n\n219\n7200s (~2 hours)\n23400s (~6.5 hours)\n27000s (~7.5 hours)\n28800s (~8 hours)\n\n\n\n\n\n\n\n\n\nextract_states() & summarize_numeric()\nIf your interest in states centers on individual occurrences - for example, how often a state occurred, how long each episode persisted, or when episodes began - use the following tools. extract_states() returns an occurrence‑level table (one row per episode) with start/end times and durations; summarize_numeric() then aggregates those episodes into concise metrics (e.g., counts, total duration, mean/median episode length) by the grouping you specify.\n\ndataset_1day |&gt; \n  extract_states(state) |&gt; \n  summarize_numeric() |&gt; \n  gt()\n\n\n\n\n\n\n\nstate\nmean_epoch\nmean_start\nmean_end\nmean_duration\ntotal_duration\nepisodes\n\n\n\n\n216\n\n\n≥250lx\n1800\n12:45:00\n15:45:00\n10800s (~3 hours)\n21600s (~6 hours)\n2\n\n\n≤10lx\n1800\n14:00:00\n16:00:00\n7200s (~2 hours)\n14400s (~4 hours)\n2\n\n\n≤1lx\n1800\n01:15:00\n07:45:00\n23400s (~6.5 hours)\n23400s (~6.5 hours)\n1\n\n\nNA\n1800\n17:27:00\n14:09:00\n5400s (~1.5 hours)\n27000s (~7.5 hours)\n5\n\n\n218\n\n\n≥250lx\n1800\n14:00:00\n14:30:00\n1800s (~30 minutes)\n3600s (~1 hours)\n2\n\n\n≤10lx\n1800\n15:45:00\n04:30:00\n2700s (~45 minutes)\n5400s (~1.5 hours)\n2\n\n\n≤1lx\n1800\n11:45:00\n15:45:00\n14400s (~4 hours)\n28800s (~8 hours)\n2\n\n\nNA\n1800\n12:25:00\n16:55:00\n16200s (~4.5 hours)\n48600s (~13.5 hours)\n3\n\n\n219\n\n\n≥250lx\n1800\n11:45:00\n12:45:00\n3600s (~1 hours)\n7200s (~2 hours)\n2\n\n\n≤10lx\n1800\n17:25:00\n11:35:00\n7800s (~2.17 hours)\n23400s (~6.5 hours)\n3\n\n\n≤1lx\n1800\n00:45:00\n08:15:00\n27000s (~7.5 hours)\n27000s (~7.5 hours)\n1\n\n\nNA\n1800\n11:55:00\n14:35:00\n9600s (~2.67 hours)\n28800s (~8 hours)\n3"
  },
  {
    "objectID": "beginner.html#its-a-wrap",
    "href": "beginner.html#its-a-wrap",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "It’s a wrap",
    "text": "It’s a wrap\nThis concludes the first part of the LightLogR tutorial. We hope it has given you a nice introduction to the package and convinced you to try it out with your own data and in your local installation. For more on LightLogR, we recommend the documentation page. If you want to stay up to date with the development of the package, you can sign up to our LightLogR mailing list."
  },
  {
    "objectID": "beginner.html#footnotes",
    "href": "beginner.html#footnotes",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you are new to the R language or want a great introduction to R for data science, we can recommend the free online book R for Data Science (second edition) by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund.↩︎\nGuidolin, C., Zauner, J., & Spitschan, M., (2025). Personal light exposure dataset for Tuebingen, Germany (Version 1.0.0) [Data set]. URL: https://github.com/MeLiDosProject/GuidolinEtAl_Dataset_2025. DOI: doi.org/10.5281/zenodo.16895188↩︎"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "Attribution 4.0 International\n\n=======================================================================\n\nCreative Commons Corporation (\"Creative Commons\") is not a law firm and\ndoes not provide legal services or legal advice. Distribution of\nCreative Commons public licenses does not create a lawyer-client or\nother relationship. Creative Commons makes its licenses and related\ninformation available on an \"as-is\" basis. Creative Commons gives no\nwarranties regarding its licenses, any material licensed under their\nterms and conditions, or any related information. Creative Commons\ndisclaims all liability for damages resulting from their use to the\nfullest extent possible.\n\nUsing Creative Commons Public Licenses\n\nCreative Commons public licenses provide a standard set of terms and\nconditions that creators and other rights holders may use to share\noriginal works of authorship and other material subject to copyright\nand certain other rights specified in the public license below. The\nfollowing considerations are for informational purposes only, are not\nexhaustive, and do not form part of our licenses.\n\n     Considerations for licensors: Our public licenses are\n     intended for use by those authorized to give the public\n     permission to use material in ways otherwise restricted by\n     copyright and certain other rights. Our licenses are\n     irrevocable. Licensors should read and understand the terms\n     and conditions of the license they choose before applying it.\n     Licensors should also secure all rights necessary before\n     applying our licenses so that the public can reuse the\n     material as expected. Licensors should clearly mark any\n     material not subject to the license. This includes other CC-\n     licensed material, or material used under an exception or\n     limitation to copyright. More considerations for licensors:\n    wiki.creativecommons.org/Considerations_for_licensors\n\n     Considerations for the public: By using one of our public\n     licenses, a licensor grants the public permission to use the\n     licensed material under specified terms and conditions. If\n     the licensor's permission is not necessary for any reason--for\n     example, because of any applicable exception or limitation to\n     copyright--then that use is not regulated by the license. Our\n     licenses grant only permissions under copyright and certain\n     other rights that a licensor has authority to grant. Use of\n     the licensed material may still be restricted for other\n     reasons, including because others have copyright or other\n     rights in the material. A licensor may make special requests,\n     such as asking that all changes be marked or described.\n     Although not required by our licenses, you are encouraged to\n     respect those requests where reasonable. More_considerations\n     for the public:\n    wiki.creativecommons.org/Considerations_for_licensees\n\n=======================================================================\n\nCreative Commons Attribution 4.0 International Public License\n\nBy exercising the Licensed Rights (defined below), You accept and agree\nto be bound by the terms and conditions of this Creative Commons\nAttribution 4.0 International Public License (\"Public License\"). To the\nextent this Public License may be interpreted as a contract, You are\ngranted the Licensed Rights in consideration of Your acceptance of\nthese terms and conditions, and the Licensor grants You such rights in\nconsideration of benefits the Licensor receives from making the\nLicensed Material available under these terms and conditions.\n\n\nSection 1 -- Definitions.\n\n  a. Adapted Material means material subject to Copyright and Similar\n     Rights that is derived from or based upon the Licensed Material\n     and in which the Licensed Material is translated, altered,\n     arranged, transformed, or otherwise modified in a manner requiring\n     permission under the Copyright and Similar Rights held by the\n     Licensor. For purposes of this Public License, where the Licensed\n     Material is a musical work, performance, or sound recording,\n     Adapted Material is always produced where the Licensed Material is\n     synched in timed relation with a moving image.\n\n  b. Adapter's License means the license You apply to Your Copyright\n     and Similar Rights in Your contributions to Adapted Material in\n     accordance with the terms and conditions of this Public License.\n\n  c. Copyright and Similar Rights means copyright and/or similar rights\n     closely related to copyright including, without limitation,\n     performance, broadcast, sound recording, and Sui Generis Database\n     Rights, without regard to how the rights are labeled or\n     categorized. For purposes of this Public License, the rights\n     specified in Section 2(b)(1)-(2) are not Copyright and Similar\n     Rights.\n\n  d. Effective Technological Measures means those measures that, in the\n     absence of proper authority, may not be circumvented under laws\n     fulfilling obligations under Article 11 of the WIPO Copyright\n     Treaty adopted on December 20, 1996, and/or similar international\n     agreements.\n\n  e. Exceptions and Limitations means fair use, fair dealing, and/or\n     any other exception or limitation to Copyright and Similar Rights\n     that applies to Your use of the Licensed Material.\n\n  f. Licensed Material means the artistic or literary work, database,\n     or other material to which the Licensor applied this Public\n     License.\n\n  g. Licensed Rights means the rights granted to You subject to the\n     terms and conditions of this Public License, which are limited to\n     all Copyright and Similar Rights that apply to Your use of the\n     Licensed Material and that the Licensor has authority to license.\n\n  h. Licensor means the individual(s) or entity(ies) granting rights\n     under this Public License.\n\n  i. Share means to provide material to the public by any means or\n     process that requires permission under the Licensed Rights, such\n     as reproduction, public display, public performance, distribution,\n     dissemination, communication, or importation, and to make material\n     available to the public including in ways that members of the\n     public may access the material from a place and at a time\n     individually chosen by them.\n\n  j. Sui Generis Database Rights means rights other than copyright\n     resulting from Directive 96/9/EC of the European Parliament and of\n     the Council of 11 March 1996 on the legal protection of databases,\n     as amended and/or succeeded, as well as other essentially\n     equivalent rights anywhere in the world.\n\n  k. You means the individual or entity exercising the Licensed Rights\n     under this Public License. Your has a corresponding meaning.\n\n\nSection 2 -- Scope.\n\n  a. License grant.\n\n       1. Subject to the terms and conditions of this Public License,\n          the Licensor hereby grants You a worldwide, royalty-free,\n          non-sublicensable, non-exclusive, irrevocable license to\n          exercise the Licensed Rights in the Licensed Material to:\n\n            a. reproduce and Share the Licensed Material, in whole or\n               in part; and\n\n            b. produce, reproduce, and Share Adapted Material.\n\n       2. Exceptions and Limitations. For the avoidance of doubt, where\n          Exceptions and Limitations apply to Your use, this Public\n          License does not apply, and You do not need to comply with\n          its terms and conditions.\n\n       3. Term. The term of this Public License is specified in Section\n          6(a).\n\n       4. Media and formats; technical modifications allowed. The\n          Licensor authorizes You to exercise the Licensed Rights in\n          all media and formats whether now known or hereafter created,\n          and to make technical modifications necessary to do so. The\n          Licensor waives and/or agrees not to assert any right or\n          authority to forbid You from making technical modifications\n          necessary to exercise the Licensed Rights, including\n          technical modifications necessary to circumvent Effective\n          Technological Measures. For purposes of this Public License,\n          simply making modifications authorized by this Section 2(a)\n          (4) never produces Adapted Material.\n\n       5. Downstream recipients.\n\n            a. Offer from the Licensor -- Licensed Material. Every\n               recipient of the Licensed Material automatically\n               receives an offer from the Licensor to exercise the\n               Licensed Rights under the terms and conditions of this\n               Public License.\n\n            b. No downstream restrictions. You may not offer or impose\n               any additional or different terms or conditions on, or\n               apply any Effective Technological Measures to, the\n               Licensed Material if doing so restricts exercise of the\n               Licensed Rights by any recipient of the Licensed\n               Material.\n\n       6. No endorsement. Nothing in this Public License constitutes or\n          may be construed as permission to assert or imply that You\n          are, or that Your use of the Licensed Material is, connected\n          with, or sponsored, endorsed, or granted official status by,\n          the Licensor or others designated to receive attribution as\n          provided in Section 3(a)(1)(A)(i).\n\n  b. Other rights.\n\n       1. Moral rights, such as the right of integrity, are not\n          licensed under this Public License, nor are publicity,\n          privacy, and/or other similar personality rights; however, to\n          the extent possible, the Licensor waives and/or agrees not to\n          assert any such rights held by the Licensor to the limited\n          extent necessary to allow You to exercise the Licensed\n          Rights, but not otherwise.\n\n       2. Patent and trademark rights are not licensed under this\n          Public License.\n\n       3. To the extent possible, the Licensor waives any right to\n          collect royalties from You for the exercise of the Licensed\n          Rights, whether directly or through a collecting society\n          under any voluntary or waivable statutory or compulsory\n          licensing scheme. In all other cases the Licensor expressly\n          reserves any right to collect such royalties.\n\n\nSection 3 -- License Conditions.\n\nYour exercise of the Licensed Rights is expressly made subject to the\nfollowing conditions.\n\n  a. Attribution.\n\n       1. If You Share the Licensed Material (including in modified\n          form), You must:\n\n            a. retain the following if it is supplied by the Licensor\n               with the Licensed Material:\n\n                 i. identification of the creator(s) of the Licensed\n                    Material and any others designated to receive\n                    attribution, in any reasonable manner requested by\n                    the Licensor (including by pseudonym if\n                    designated);\n\n                ii. a copyright notice;\n\n               iii. a notice that refers to this Public License;\n\n                iv. a notice that refers to the disclaimer of\n                    warranties;\n\n                 v. a URI or hyperlink to the Licensed Material to the\n                    extent reasonably practicable;\n\n            b. indicate if You modified the Licensed Material and\n               retain an indication of any previous modifications; and\n\n            c. indicate the Licensed Material is licensed under this\n               Public License, and include the text of, or the URI or\n               hyperlink to, this Public License.\n\n       2. You may satisfy the conditions in Section 3(a)(1) in any\n          reasonable manner based on the medium, means, and context in\n          which You Share the Licensed Material. For example, it may be\n          reasonable to satisfy the conditions by providing a URI or\n          hyperlink to a resource that includes the required\n          information.\n\n       3. If requested by the Licensor, You must remove any of the\n          information required by Section 3(a)(1)(A) to the extent\n          reasonably practicable.\n\n       4. If You Share Adapted Material You produce, the Adapter's\n          License You apply must not prevent recipients of the Adapted\n          Material from complying with this Public License.\n\n\nSection 4 -- Sui Generis Database Rights.\n\nWhere the Licensed Rights include Sui Generis Database Rights that\napply to Your use of the Licensed Material:\n\n  a. for the avoidance of doubt, Section 2(a)(1) grants You the right\n     to extract, reuse, reproduce, and Share all or a substantial\n     portion of the contents of the database;\n\n  b. if You include all or a substantial portion of the database\n     contents in a database in which You have Sui Generis Database\n     Rights, then the database in which You have Sui Generis Database\n     Rights (but not its individual contents) is Adapted Material; and\n\n  c. You must comply with the conditions in Section 3(a) if You Share\n     all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not\nreplace Your obligations under this Public License where the Licensed\nRights include other Copyright and Similar Rights.\n\n\nSection 5 -- Disclaimer of Warranties and Limitation of Liability.\n\n  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE\n     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS\n     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF\n     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,\n     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,\n     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR\n     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,\n     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT\n     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT\n     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\n\n  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE\n     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,\n     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,\n     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,\n     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR\n     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN\n     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR\n     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR\n     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\n\n  c. The disclaimer of warranties and limitation of liability provided\n     above shall be interpreted in a manner that, to the extent\n     possible, most closely approximates an absolute disclaimer and\n     waiver of all liability.\n\n\nSection 6 -- Term and Termination.\n\n  a. This Public License applies for the term of the Copyright and\n     Similar Rights licensed here. However, if You fail to comply with\n     this Public License, then Your rights under this Public License\n     terminate automatically.\n\n  b. Where Your right to use the Licensed Material has terminated under\n     Section 6(a), it reinstates:\n\n       1. automatically as of the date the violation is cured, provided\n          it is cured within 30 days of Your discovery of the\n          violation; or\n\n       2. upon express reinstatement by the Licensor.\n\n     For the avoidance of doubt, this Section 6(b) does not affect any\n     right the Licensor may have to seek remedies for Your violations\n     of this Public License.\n\n  c. For the avoidance of doubt, the Licensor may also offer the\n     Licensed Material under separate terms or conditions or stop\n     distributing the Licensed Material at any time; however, doing so\n     will not terminate this Public License.\n\n  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public\n     License.\n\n\nSection 7 -- Other Terms and Conditions.\n\n  a. The Licensor shall not be bound by any additional or different\n     terms or conditions communicated by You unless expressly agreed.\n\n  b. Any arrangements, understandings, or agreements regarding the\n     Licensed Material not stated herein are separate from and\n     independent of the terms and conditions of this Public License.\n\n\nSection 8 -- Interpretation.\n\n  a. For the avoidance of doubt, this Public License does not, and\n     shall not be interpreted to, reduce, limit, restrict, or impose\n     conditions on any use of the Licensed Material that could lawfully\n     be made without permission under this Public License.\n\n  b. To the extent possible, if any provision of this Public License is\n     deemed unenforceable, it shall be automatically reformed to the\n     minimum extent necessary to make it enforceable. If the provision\n     cannot be reformed, it shall be severed from this Public License\n     without affecting the enforceability of the remaining terms and\n     conditions.\n\n  c. No term or condition of this Public License will be waived and no\n     failure to comply consented to unless expressly agreed to by the\n     Licensor.\n\n  d. Nothing in this Public License constitutes or may be interpreted\n     as a limitation upon, or waiver of, any privileges and immunities\n     that apply to the Licensor or You, including from the legal\n     processes of any jurisdiction or authority.\n\n\n=======================================================================\n\nCreative Commons is not a party to its public\nlicenses. Notwithstanding, Creative Commons may elect to apply one of\nits public licenses to material it publishes and in those instances\nwill be considered the “Licensor.” The text of the Creative Commons\npublic licenses is dedicated to the public domain under the CC0 Public\nDomain Dedication. Except for the limited purpose of indicating that\nmaterial is shared under a Creative Commons public license or as\notherwise permitted by the Creative Commons policies published at\ncreativecommons.org/policies, Creative Commons does not authorize the\nuse of the trademark \"Creative Commons\" or any other trademark or logo\nof Creative Commons without its prior written consent including,\nwithout limitation, in connection with any unauthorized modifications\nto any of its public licenses or any other arrangements,\nunderstandings, or agreements concerning use of licensed material. For\nthe avoidance of doubt, this paragraph does not form part of the\npublic licenses.\n\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "index.html#choose-your-track",
    "href": "index.html#choose-your-track",
    "title": "Welcome to the LightLogR Course Series",
    "section": "Choose your track",
    "text": "Choose your track\nLive tutorials run a self‑contained version of R in your browser - no setup required - with only minor functional limitations. Static tutorials provide the complete script exactly as you would run it in a local R installation.\nBeginner (live) Beginner (static)\nTo come soon:\nAdvanced (live) Advanced (static)"
  },
  {
    "objectID": "index.html#what-youll-do",
    "href": "index.html#what-youll-do",
    "title": "Welcome to the LightLogR Course Series",
    "section": "What you’ll do",
    "text": "What you’ll do\n\nWalk through the LightLogR workflow: import → (pre-)processing → visualisation → metrics.\nSense‑check data quickly with tidy, reproducible steps and overview plots.\nHandle common issues (gaps, irregular sampling, zero inflation) with principled defaults.\nAdd photoperiod information and derive interpretable summaries.\n\nFor more information on the two tracks, have a look at the course flyer\n\n\n\n\n\n\nNote\n\n\n\nPrerequisites\n\nBeginner: Basic familiarity with R; no prior LightLogR experience required.\nAdvanced: Comfortable with tidy workflows and either completion of the beginner track or equivalent LightLogR experience."
  },
  {
    "objectID": "index.html#how-do-i-get-started",
    "href": "index.html#how-do-i-get-started",
    "title": "Welcome to the LightLogR Course Series",
    "section": "How do I get started?",
    "text": "How do I get started?\nThe tutorials are open to give them a go at any time. Try the live tutorials to forego any setup, or download the scripts from the static tutorials alongside the data from the GitHub repository for the full experience.\nWe are also offering free, regular webinars on the topic of Open and reproducible analysis of light exposure and visual experience data using `LightLogR. Have a look at the course flyer and register here."
  },
  {
    "objectID": "beginner-live.html#preface",
    "href": "beginner-live.html#preface",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "1 Preface",
    "text": "1 Preface\nWearables are increasingly used in research because they combine personalized, high‑temporal‑resolution measurements with outcomes related to well‑being and health. In sleep research, wrist‑worn actimetry is long established. As circadian factors gain prominence across disciplines, interest in personal light exposure has grown, spurring a variety of new devices, form factors, and sensor technologies. This trend also brings many researchers into settings where data from wearables must be ingested, processed, and analyzed. Beyond circadian science, measurements of light and optical radiation are central to UV‑related research and to questions of ocular health and development.\nLightLogR is designed to facilitate the principled import, processing, and visualization of such wearable‑derived data. This document offers an accessible entry point to LightLogR via a self‑contained analysis script that you can modify to familiarize yourself with the package. Full documentation of LightLogR’s features is available on the documentation page, including numerous tutorials.\nThis document is intended for researchers with no prior experience using LightLogR, and assumes general familiarity with the R statistical software, ideally in a data‑science context1."
  },
  {
    "objectID": "beginner-live.html#how-this-page-works",
    "href": "beginner-live.html#how-this-page-works",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "2 How this page works",
    "text": "2 How this page works\nThis document runs a self‑contained version of R completely in your browser2. No setup or installation is required.\nTry it for yourself by making a quick visualization of a sample dataset that comes preloaded with the package. It contains six days of data from a participant, with concurrent measurements of environmental light exposure at the university rooftop. You can play with the arguments to see how it changes the output. As soon as as webR has finished loading in the background, the Run Code button on code cells will become available.\n\n\n\n\n\n\n\n\nYou can execute the same script in a traditional R environment, but this browser‑based approach has several advantages:\n\nYou can get started in seconds, avoiding configuration differences across machines and getting to the interesting part quickly.\nUnlike a static tutorial, you can modify code to test the effects of different arguments and functions and receive immediate feedback.\nBecause everything runs locally in your browser, there are no additional server‑side security risks and minimal network‑related slowdowns.\n\nThis approach also comes with a few drawbacks:\n\nR and all required packages are loaded every time you load the page. If you close the page or navigate elsewhere in the same tab, webR must be re‑initialized and your session state is lost.\nCertain functions do not behave as they would in a traditional runtime. For example, saving plot images directly to your local machine (e.g., with ggsave()) is not supported. If you need these capabilities, run the script on your local R installation. In most cases, however, you can interact with the code as you would locally. Known cases where webR does not produce the desired output are marked specifically in this script and static images of outputs are displayed.\nAfter running a command for more than 30 seconds, each code cell will go into a time out. If that happens on your browser, try reducing the complexity of commands or choose the local installation.\nDepending on your browser and system settings, functionality or output may differ. Common differences include default fonts and occasional plot background colors. If you encounter an issue, please describe it in detail—along with your system information (hardware, OS, browser)—in the issues section of the GitHub repository. This helps us to improve your experience moving forward."
  },
  {
    "objectID": "beginner-live.html#installation",
    "href": "beginner-live.html#installation",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "3 Installation",
    "text": "3 Installation\nLightLogR is hosted on CRAN, which means it can easily be installed from any R console through the following command:\n\n\n\n\n\n\n\n\nAfter installation, it becomes available for the current session by loading the package. We also require a number of packages. Most are automatically downloaded with LightLogR, but need to be loaded separately. Some might have to be installed separately on your local machine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat is all we need to get started."
  },
  {
    "objectID": "beginner-live.html#import",
    "href": "beginner-live.html#import",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "4 Import",
    "text": "4 Import\n\n4.1 File formats\nTo work with LightLogR, we need some data from wearables. Below are screenshots from three example formats to highlight the structure and differences. You can enlarge by clicking at them.\n\n\n\n\n\n\nActLumus file structure\n\n\n\n\n\n\n\nSpeccy file structure\n\n\n\n\n\n\n\nnanoLambda file structure\n\n\n\n\n\n\n\n4.2 Importing a file\nThese files must be loaded into the active session in a tidy format—each variable in its own column and each observation in its own row. LightLogR’s device‑specific import functions take care of this transformation. Each function requires:\n\nfilenames and paths to the wearable export files\nthe time zone in which the data were collected\n(optional) participant identifiers\n\nWe begin with a dataset bundled with the package, recorded with the ActLumus device. The data were collected in Tübingen, Germany, so the correct time zone is Europe/Berlin.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe import function also provides rich summary information about the dataset—such as the time span covered, sampling intervals, and an overview plot. Most import settings are configurable. To learn more, consult the function documentation online or via ?import. For a quick visual overview of the data across days, draw a timeline with gg_days().\n\n\n\n\n\n\n\n\nWe will go into much more detail about visualizations in the sections below.\n\n\n4.3 Importing from a different device\nEach device exports data in its own format, necessitating device‑specific handling. LightLogR includes import wrapper functions for many devices. You can retrieve the list supported by your installed version with the following function:\n\n\n\n\n\n\n\n\nWe will now import from two other devices to showcase the differences.\n\n4.3.1 Speccy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 nanoLambda\n\n\n\n\n\n\n\n\nIf we try to visualize this dataset as we have done above, we get an error.\n\n\n\n\n\n\n\n\nThis is because many LightLogR functions default to the melanopic EDI variable3. However, the nanoLambda export does not include this variable. Therefore, we must explicitly specify which variable to display. Let’s inspect the available variables:\n\n\n\n\n\n\n\n\nYou can choose any numeric variable; here, we’ll use Melanopic_Lux, which is similar—though not identical—to melanopic EDI. To identify which argument to adjust, consult the [function documentation]:\n\n\n\n\n\n\n\n\nUse the y.axis argument to select the variable. Also update the axis title via y.axis.label; otherwise the default label will refer to melanopic EDI.\nBecause this dataset spans only a short interval of about 9 minutes, we’ll visualize it with gg_day(), which uses clock time on the x‑axis. There are a few other differences to gg_days(), which we will see in the sections below.\n\n\n\n\n\n\n\n\nIn summary, importing from different devices is typically as simple as specifying the device name. Some devices require additional arguments; consult the ?import help for details.\n\n\n\n4.4 Importing more than one file\nIn typical studies, you’ll work with multiple participants, and importing each file individually is cumbersome. LightLogR supports batch imports; simply pass multiple files to the import function. In this tutorial, we’ll use three files from three participants, all drawn from the open‑access personal light‑exposure dataset by Guidolin et al. 20254. All data were collected with the ActLumus device type.\nWhen importing multiple files, keep the following in mind:\n\nAll files must originate from the same device type, share the same export structure, and use the same time‑zone specification. If they differ, import them separately.\nBe deliberate about participant‑ID assignment. The manual.id argument used above would assign the same ID to all imported data in a batch. If a file contains a column specifying the Id, you can point to that column; more often, the identifier is encoded in the filename. If you omit ID arguments, the filename is used as Id by default. Because filenames are often verbose, you will typically extract only the participant code. In our three example files, the relevant IDs are 216, 218, and 219.\n\n\n\n\n\n\n\n\n\nIf filenames follow a consistent pattern, you can instruct the import function to extract only the participant code from each name. In our case, the first three digits encode the ID. We can specify this with a regular expression: ^(\\d{3}). This pattern matches the first three digits at the start of the filename and captures them (^ = start of string, \\d = digit, {3} = exactly three, (& ) = encloses the part of the pattern we actually want). If you’re not familiar with regular expressions, they can look like a jumble of ASCII characters, but they succinctly express patterns. Large language models are quite good at proposing regexes and explaining their components, so consider prompting one when you need a new pattern. With that, we can import our files.\n\n\n\n\n\n\n\n\nThe overview plot is now more informative: it shows how the datasets align across time and highlights extended gaps due to missing data. We will return to the terminology of implicit missingness shortly.\n\n\n\n\n\n\n\n\nDirect plotting highlights the extended gaps in the recordings. We’ll apply a package function that removes days with insufficient coverage to address this. For now, we can ignore the details: any participant‑day with more than 80% missing data will be excluded.\n\n\n\n\n\n\n\n\nThat concludes the import section of the tutorial; next, we turn to visualization functions. For simplicitly, we will only carry a small selection of variables forward. That increases the calculation speed of many functions. Feel free to choose a different set of variables.\n\n\n\n\n\n\n\n\n\n\n4.5 How to find the correct time zone name?\nA final note on imports: the function accepts only valid IANA time‑zone identifiers. You can retrieve the full list (with exact spellings) using:"
  },
  {
    "objectID": "beginner-live.html#basic-visualizations",
    "href": "beginner-live.html#basic-visualizations",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "5 Basic Visualizations",
    "text": "5 Basic Visualizations\nVisualization is central to exploratory data analysis and to communicating results in publications and presentations. LightLogR provides a suite of plotting functions built on ggplot2 and the Grammar of Graphics. As a result, the plots are composable, flexible, and straightforward to modify.\n\n5.1 gg_days()\ngg_days() displays a timeline per each Id. It constrains the x‑axis to complete days and, by default, uses a line geometry. The function works best for up to a handful of Id’s and 1-2 weeks of data at most.\n\n\n\n\n\n\n\n\n\n\n5.2 gg_day()\ngg_day() complements gg_days() by focusing on individual calendar days. By default, it places all observations from a selected day into a single panel, regardless of source. This layout is configurable. For readability, gg_day() works best with ~1–4 days of data (at most about a week) to keep plot height manageable.\n\n\n\n\n\n\n\n\n\n\n5.3 gg_overview()\ngg_overview() is invoked automatically by the import function but can also be called independently and customized. By default, each Id appears as a separate row on the y‑axis. For longitudinal datasets with large gaps between recordings, you can group observations (e.g., by a session variable) to distinguish distinct measurement periods (see figure below). The function works nice for many participants and long collection periods, by setting their recording periods in relation. By default, it will also show times of implicitly missing data.\n\n\n\n\n\n\n\n\n\n\n\nGrouping the data by Id and measurement session provides easy overviews for longitudinal datasets\n\n\n\n\n5.4 gg_heatmap()\ngg_heatmap() renders one calendar day per row within each data‑collection period. It is well‑suited to long monitoring spans and scales effectively to many participants. To highlight patterns that cross midnight, it supports a doubleplot option that displays a duplicate of the day, or the next day with an offset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5 What about non-light variables?\nLightLogR is optimized for wearable light sensors and selects sensible defaults: for example, melanopic EDI (when available) and settings suited to typical light‑exposure distributions. Nevertheless, the functions are measurement‑agnostic and can be applied to non‑light variables. Consult the function documentation to see which arguments to adjust for your variable of interest. For example, here we plot an activity variable:\n\n\n\n\n\n\n\n\nWhile beyond the scope of this tutorial, there are many research disciplines that work with visual experience data, which covers many different domains: Light, Spectrum, Distance, Spatial resolution, ….\nIf you are interested in those domains, we recommend our tutorial specifically for the more comprehensive but also more complex datasets in the visual experience domains."
  },
  {
    "objectID": "beginner-live.html#validation",
    "href": "beginner-live.html#validation",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "6 Validation",
    "text": "6 Validation\nCurrently, LightLogR’s validation aims to ensure a regular, uninterrupted time series for each participant. Additional features are planned.\nThe figures below summarize the gap terminology used in LightLogR and illustrate how gap_handler() fills implicit missing data.\n\n\n\n\n\n\nTerminology of gaps in LightLogR\n\n\n\n\n\n\n\ngap_handler() identifies the time series’ dominant epoch (the most common sampling interval) and fills NA entries between the first and last observation. By default, no observations are dropped, so irregular samples are preserved.\n\n\n\n\n\nTo quickly assess whether a dataset contains (implicit) gaps or irregular sampling, use the following diagnostic helpers:\n\n\n\n\n\n\n\n\nWe can then quickly visualize where these issues occur within the affected days.\n\n\n\n\n\n\n\n\n\n\n\nTo generate this figure yourself, please execute the static R script\n\n\nThis function can be slow when a dataset contains many gaps or irregular samples. If needed, pre‑filter the data or adjust the function’s arguments.\nIn our example, we identify eight participant‑days with gaps:\n\nThree straightforward cases: data collection ends around noon on Monday, leaving the remainder of the day missing. By default, the function evaluates complete calendar days (this is configurable). These days only require converting implicit gaps into explicit missing values.\nTwo pre‑trial snippets: brief measurements occur on the Friday or Monday preceding the trial—likely test recordings. These days are outside the study window and should be removed entirely.\nThree early irregularities: irregular sampling appears shortly after data collection starts. This most likely reflects a test recording immediately before the device was handed to the participant. Trimming this initial segment eliminates the irregularity and the rest of the day can be changed to explicit missingness.\n\n\n6.1 Preparing the dataset\nThere are several ways to address these issues. We will showcase three in the next sections.\n\n6.1.1 1. Set the maximum length of the dataset.\nIf the study follows a fixed‑length protocol, you can enforce a maximum observation window (e.g., 7 days) by trimming from the beginning so that each participant’s series has the same duration. This approach preserves participant‑specific end times, which must meaningfully reflect protocol completion; otherwise, you risk cutting away valid data.\n\n\n\n\n\n\n\n\nThe remaining gaps are simple start‑ and end‑day truncations.\n\n\n6.1.2 2. Remove the first values from the dataset\nYou can remove a fixed number of observations from the beginning of each participant’s series. This approach is helpful when the exact total measurement duration is not critical—for example, to discard brief pre‑trial test recordings or initial device‑stabilization periods.\n\n\n\n\n\n\n\n\nThe results are similarly effective.\n\n\n6.1.3 3. Trim with a list\nThe most robust way to enforce sensible measurement windows is to supply a table of trial start and end timestamps (per participant) and filter the time series accordingly. In this tutorial we create that table on the fly; in practice, it is typically stored in a CSV or Excel file. The add_states() function provides an effective interface between the two datasets: it aligns by identifier and time, adds state information (e.g., “in‑trial”), and enables precise trimming. Ensure that the identifying variables (e.g., Id) are named identically across files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2 gap_table()\nWe can summarize each dataset’s regularity and missingness in a table. Note that this function may be slow when many gaps are present.\n\n\n\n\n\n\n\n\n\n\n\nTo generate this table yourself, please execute the static R script\n\n\n\n\n6.3 gap_handler()\nApproximately 13% of the missing data are implicit—they arise from truncated start and end days. It is good practice to make these gaps explicit. Use gap_handler(full.days = TRUE) to fill implicit gaps to full‑day regularity. Then verify the result with gap_table(), the diagnostic helpers, and a follow‑up visualization:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo generate this table yourself, please execute the static R script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4 remove_partial_data()\nIt is often necessary to set missingness thresholds at different levels (hour, day, participant). Typical questions include:\n\nHow much data may be missing within an hour before that hour is excluded?\nHow much data may be missing from a day before that day is excluded?\nHow much data may be missing for a participant before excluding them from further analyses?\n\nremove_partial_data() addresses these questions. It evaluates each group (by default, Id) and quantifies missingness either as an absolute duration or a relative proportion. Groups that exceed the specified threshold are discarded. A useful option is by.date, which performs the thresholding per calendar day (for removal) while leaving the output grouping unchanged. Note that missingness is determined by the amount of data points in each group, relative to NA values.\nFor this tutorial, we will remove any day with more than one hour of missing data—this effectively drops both partial Mondays:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy did we just spend all this time handling gaps and irregularities on the Mondays only to remove them afterward?\n\n\n\nNot all datasets are this straightforward. Deciding whether a day should be included in the analysis should come after ensuring the data are aligned to a regular, uninterrupted time series. Regularization makes diagnostics meaningful and prevents threshold rules from behaving unpredictably.\nMoreover, there are different frameworks for grouping personal light‑exposure data. In this tutorial we focus on calendar dates and 24‑hour days. Other frameworks group differently. For example, anchoring to sleep–wake cycles—under which both Mondays might still contain useful nocturnal data. Harmonizing first ensures those alternatives remain viable even if calendar‑day summaries are later excluded."
  },
  {
    "objectID": "beginner-live.html#metrics",
    "href": "beginner-live.html#metrics",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "7 Metrics",
    "text": "7 Metrics\nMetrics form the second major pillar of LightLogR, alongside visualization. The literature contains many light‑exposure metrics; LightLogR implements a broad set of them behind a uniform, well‑documented interface. The currently available metrics are:\n\n\n\n\n\n\n\n\n\nMetric Family\nSubmetrics\nNote\nDocumentation\n\n\n\n\nBarroso\n7\n\nbarroso_lighting_metrics()\n\n\nBright-dark period\n4x2\nbright / dark\nbright_dark_period()\n\n\nCentroid of light exposure\n1\n\ncentroidLE()\n\n\nDose\n1\n\ndose()\n\n\nDisparity index\n1\n\ndisparity_index()\n\n\nDuration above threshold\n3\nabove, below, within\nduration_above_threshold()\n\n\nExponential moving average (EMA)\n1\n\nexponential_moving_average()\n\n\nFrequency crossing threshold\n1\n\nfrequency_crossing_threshold()\n\n\nIntradaily Variance (IV)\n1\n\nintradaily_variability()\n\n\nInterdaily Stability (IS)\n1\n\ninterdaily_stability()\n\n\nMidpoint CE (Cumulative Exposure)\n1\n\nmidpointCE()\n\n\nnvRC (Non-visual circadian response)\n4\n\nnvRC(), nvRC_circadianDisturbance(), nvRC_circadianBias(), nvRC_relativeAmplitudeError()\n\n\nnvRD (Non-visual direct response)\n2\n\nnvRD(), nvRD_cumulative_response()\n\n\nPeriod above threshold\n3\nabove, below, within\nperiod_above_threshold()\n\n\nPulses above threshold\n7x3\nabove, below, within\npulses_above_threshold()\n\n\nThreshold for duration\n2\nabove, below\nthreshold_for_duration()\n\n\nTiming above threshold\n3\nabove, below, within\ntiming_above_threshold()\n\n\nTotal:\n\n\n\n\n\n17 families\n62 metrics\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLightLogR supports a wide range of metrics across different metric families. You can find the full documentation of metrics functions in the reference section. There is also an overview article on how to use Metrics.\nIf you would like to use a metric you don’t find represented in LightLogR, please contact the developers. The easiest and most trackable way to get in contact is by opening a new issue on our Github repository.\n\n\n\n7.1 Principles\nEach metric function operates on vectors. Although the main argument is often named Light.vector, the name is conventional - the function will accept any variable you supply. All metric functions are thoroughly documented, with references to their intended use and interpretation.\nWhile we don’t generally recommend it, you can pass a raw vector directly to a metric function. For example, to compute Time above 250 lx melanopic EDI, you could run:\n\n\n\n\n\n\n\n\nHowever, that single result is not very informative - it aggregates across all participants and all days. To recover the total recorded duration, recompute the complementary metric: Time below 250 lx melanopic EDI. This should approximate the full two weeks and four days of data when evaluated over the whole dataset:\n\n\n\n\n\n\n\n\nThe problem is amplified for metrics defined at the day scale (or shorter). For example, the brightest 10 hours (M10) is computed within each 24‑hour day using a consecutive 10‑hour window—so applying it to a pooled, cross‑day vector is almost meaningless:\n\n\n\n\n\n\n\n\nThe resulting value - although computationally valid - is substantively meaningless: it selects the single brightest 10‑hour window across all participants, rather than computing M10 per participant per day. In addition, two time series (218 & 219) overlap in time, which violates the assumption of a single, regularly spaced series and can produce errors. Hence the Warning: Time.vector is not regularly spaced. Calculated results may be incorrect!\nAccordingly, metric functions should be applied within tidy groups (e.g., by Id and by calendar Date), not to a pooled vector. You can achieve this with explicit for‑loops or, preferably, a tidy approach using dplyr (e.g., group_by()/summarise() or nest()/map()). We recommend the latter.\n\n\n7.2 Use of summarize()\nWrap the metric inside a dplyr summarise()/summarize() call, supply the grouped dataset, and set as.df = TRUE. This yields a tidy, one‑row‑per‑group result (e.g., per Id). For example, computing interdaily stability (IS):\n\n\n\n\n\n\n\n\nTo compute multiple metrics at once, include additional expressions inside the summarize() call. For instance, add Time above 250 lx melanopic EDI alongside IS:\n\n\n\n\n\n\n\n\nFor finer granularity, add additional grouping variables before summarizing—for example, group by calendar Date to compute metrics per participant–day:\n\n\n\n\n\n\n\n\nWe can further condense this:\n\n\n\n\n\n\n\n\nThat’s all you need to get started with metric calculation in LightLogR. While advanced metrics involve additional considerations, this tidy grouped workflow will take you a long way."
  },
  {
    "objectID": "beginner-live.html#photoperiod",
    "href": "beginner-live.html#photoperiod",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "8 Photoperiod",
    "text": "8 Photoperiod\nPhotoperiod is a key covariate in many analyses of personal light exposure. LightLogR includes utilities to derive photoperiod information with minimal effort. All you need are geographic coordinates in decimal degrees (latitude, longitude); functions will align photoperiod features to your time series. Provide coordinates in standard decimal format (e.g., 48.52, 9.06):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1 Photoperiod in visualizations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2 Data\nPhotoperiod features make it easy to split data into day and night states—for example, to compute metrics by phase. The number_states() function places a counter each time the state changes, effectively numbering successive day and night episodes. Grouping by these counters then allows you to calculate metrics for individual days and nights:\n\n\n\n\n\n\n\n\nThis yields the average brightest 1‑hour period for each participant, separately for day and night. Notably, the participant with the highest daytime brightness also shows the lowest nighttime brightness, and vice versa."
  },
  {
    "objectID": "beginner-live.html#distribution-of-light-exposure",
    "href": "beginner-live.html#distribution-of-light-exposure",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "9 Distribution of light exposure",
    "text": "9 Distribution of light exposure\nPersonal light‑exposure data exhibit a characteristic distribution (see figure): they are strongly right‑skewed—approximately log‑normal—and contain many zeros (i.e., zero‑inflation).\n\n\n\nDistribution of light exposure in the environment and for a participant, both at night and day\n\n\nConsequently, the arithmetic mean is not a representative summary for these data. We can visualize this by placing common location metrics on the distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo better characterize zero‑inflated, right‑skewed light data, use log_zero_inflated(). The function adds a small constant (ε) to every observation before taking logs, making the transform well‑defined at zero. Choose ε based on the device’s measurement resolution/accuracy; for wearables spanning roughly 1–10^5 lx, we recommend ε = 0.1 lx. The inverse, exp_zero_inflated(), returns values to the original scale by exponentiating and then subtracting the same ε. The default basis for these functions is 10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1 Log zero-inflated with metrics\nWhen computing averaging metrics, apply the transformation explicitly to the variable you pass to the metric. This ensures the statistic is computed on the intended scale and makes your code easy to audit later.\nFor the zero‑inflated log approach, transform before averaging and (if desired) back‑transform for reporting:"
  },
  {
    "objectID": "beginner-live.html#summaries",
    "href": "beginner-live.html#summaries",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "10 Summaries",
    "text": "10 Summaries\nSummary helpers provide fast, dataset‑wide overviews. Existing examples include gap_table() (tabular diagnostics) and gg_overview() (visual timeline). In the next release, two higher‑level tools are planned: grand_overview() (a dataset‑level summary plot) and light_summary_table() (a table of key exposure metrics). These are not available in LightLogR 0.9.3 but are slated for an upcoming release and are shown in the next two sections as sneak previews. In keeping with LightLogR’s design, they will have straightforward interfaces and play well with grouped/tidy workflows.\n\n10.1 Summary plot\n\n\n\n\n\n\n\n\n\n\n\nTo generate this figure yourself, please execute the static R script\n\n\n\n\n10.2 Summary table\n\n\n\n\n\n\n\n\n\n\n\nTo generate this table yourself, please execute the static R script"
  },
  {
    "objectID": "beginner-live.html#processing-states",
    "href": "beginner-live.html#processing-states",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "11 Processing & states",
    "text": "11 Processing & states\nLightLogR contains many functions to manipulate, expand, or condense datasets. We will highlight the most important ones.\n\n11.1 aggregate_Datetime()\naggregate_Datetime() is a general‑purpose resampling utility that bins observations into fixed‑duration intervals and computes a summary statistic per bin. It is intentionally opinionated, providing sensible defaults (e.g., mean for numeric columns and mode for character/factor columns), but all summaries are configurable and additional metrics can be requested. Use it as a lightweight formatter to change the effective measurement interval after the fact (e.g., re‑epoching from 10 s to 1 min).\n\n\n\n\n\n\n\n\n\n\n11.2 aggregate_Date()\naggregate_Date() is a companion function that collapses each group into a single 24‑hour profile, optionally re‑epoching the data in the process. It is well‑suited to very large datasets when you need an overview of the average day. It applies the same summarization rules as aggregate_Datetime() and is equally configurable to your requirements:\n\n\n\n\n\n\n\n\n\n\n11.3 gg_doubleplot()\naggregate_Date() pairs well with gg_doubleplot(), which duplicates each day with an offset to reveal patterns that span midnight. While it can be applied to any dataset, use it on only a handful of days at a time to keep plots readable. If the dataset it is called on contains more than one day gg_doubleplot() defaults to displaying the next day instead of duplicating the same day.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.4 Beyond inital variables\nBoth aggregate_Datetime() and aggregate_Date() allow for the calculation of additional metrics within their respective bins. One use case is to gauge the spread of the data within certain times. A simple approach is to plot the minimum and maximum value of a dataset that was condensed to a single day.\n\n\n\n\n\n\n\n\n\n\n11.5 States\nStates, in the context of LightLogR, means any non-numeric variable. Those can be part of the dataset, be calculated from the dataset (e.g., mel EDI &gt;= 250 lx), or added from an external source. We showcase some capabilities by dividing the dataset into sections by the Brown et al. (2022) recommendations for healthy lighting, using the Brown_cut() function.\n\n\n\n\n\n\n\n\n\n11.5.1 gg_state()\ngg_state() augments an existing plot by adding background rectangles that mark state intervals. When multiple states are present, map them to distinct fills (or colors) to improve readability.\n\n\n\n\n\n\n\n\n\n\n\n11.6 durations()\nIf you need a numeric summary of states, durations() computes the total time spent in each state per grouping (e.g., by Id, by day). With a small reshaping step, you can produce a tidy table showing the average duration each participant spends in each state:\n\n\n\n\n\n\n\n\n\n\n11.7 extract_states() & summarize_numeric()\nIf your interest in states centers on individual occurrences - for example, how often a state occurred, how long each episode persisted, or when episodes began - use the following tools. extract_states() returns an occurrence‑level table (one row per episode) with start/end times and durations; summarize_numeric() then aggregates those episodes into concise metrics (e.g., counts, total duration, mean/median episode length) by the grouping you specify."
  },
  {
    "objectID": "beginner-live.html#its-a-wrap",
    "href": "beginner-live.html#its-a-wrap",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "12 It’s a wrap",
    "text": "12 It’s a wrap\nThis concludes the first part of the LightLogR tutorial. We hope it has given you a nice introduction to the package and convinced you to try it out with your own data and in your local installation. For more on LightLogR, we recommend the documentation page. If you want to stay up to date with the development of the package, you can sign up to our LightLogR mailing list."
  },
  {
    "objectID": "beginner-live.html#footnotes",
    "href": "beginner-live.html#footnotes",
    "title": "Open and reproducible analysis of light exposure and visual experience data (Beginner)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you are new to the R language or want a great introduction to R for data science, we can recommend the free online book R for Data Science (second edition) by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund.↩︎\nIf you want to know more about webR and the Quarto-live extension that powers this document, you can visit the documentation page↩︎\nmelanopic equivalent daylight-illuminance, CIE S026:2018↩︎\nGuidolin, C., Zauner, J., & Spitschan, M., (2025). Personal light exposure dataset for Tuebingen, Germany (Version 1.0.0) [Data set]. URL: https://github.com/MeLiDosProject/GuidolinEtAl_Dataset_2025. DOI: doi.org/10.5281/zenodo.16895188↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About / Funding",
    "section": "",
    "text": "This online course was developed by the Translational Sensory & Circadian Neuroscience Unit."
  },
  {
    "objectID": "about.html#funding",
    "href": "about.html#funding",
    "title": "About / Funding",
    "section": "Funding",
    "text": "Funding\nLightLogR’s development is supported by the following projects and funders:"
  }
]